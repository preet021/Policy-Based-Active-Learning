{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 784) (1000, 784)\n",
      "(6000,) (1000,)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    num_points = len(lines)\n",
    "    dim_points = 28 * 28\n",
    "    data = np.empty((num_points, dim_points))\n",
    "    labels = np.empty(num_points)\n",
    "    \n",
    "    for ind, line in enumerate(lines):\n",
    "        num = line.split(',')\n",
    "        labels[ind] = int(num[0])\n",
    "        data[ind] = [ int(x) for x in num[1:] ]\n",
    "        \n",
    "    return (data, labels)\n",
    "\n",
    "train_data, train_labels = read_data(\"sample_train.csv\")\n",
    "test_data, test_labels = read_data(\"sample_test.csv\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "print(type(test_data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = self.fc2(t)\n",
    "        return t\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(self.fc3(x))\n",
    "\n",
    "def train(model, optimizer, X, criterion=nn.NLLLoss()):\n",
    "    epochs = 1\n",
    "    batch_size = 8\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        num_batches = 0\n",
    "        order = np.copy(X)\n",
    "        np.random.shuffle(order)\n",
    "        i = 0\n",
    "        while i < len(X):\n",
    "            j = min(i + batch_size, len(X))\n",
    "#             print(order[i:j])\n",
    "            images = train_data[order[i:j], :]\n",
    "            labels = torch.Tensor(train_labels[order[i:j]]).long()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(torch.from_numpy(images).float())\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            i += batch_size\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss / num_batches))\n",
    "\n",
    "def predict(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.from_numpy(test_data).float())\n",
    "    model.train()\n",
    "    softmax = torch.exp(torch.Tensor(output))\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    return accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 started\n",
      "Epoch 0 - Training loss: 9.494784355163574\n",
      "Acc 0.108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 8.746197819709778\n",
      "Acc 0.115\n",
      "Epoch 0 - Training loss: 8.639788707097372\n",
      "Acc 0.132\n",
      "Epoch 0 - Training loss: 5.517461180686951\n",
      "Acc 0.158\n",
      "Epoch 0 - Training loss: 3.1371501326560973\n",
      "Acc 0.191\n",
      "Epoch 0 - Training loss: 2.3503908862670264\n",
      "Acc 0.227\n",
      "Epoch 0 - Training loss: 1.9444488840443748\n",
      "Acc 0.272\n",
      "Epoch 0 - Training loss: 1.4046201333403587\n",
      "Acc 0.304\n",
      "Epoch 0 - Training loss: 1.2241539087974362\n",
      "Acc 0.338\n",
      "Epoch 0 - Training loss: 1.0489060312509537\n",
      "Acc 0.372\n",
      "Epoch 0 - Training loss: 0.8908535706048663\n",
      "Acc 0.415\n",
      "Epoch 0 - Training loss: 1.0498574102918308\n",
      "Acc 0.447\n",
      "Epoch 0 - Training loss: 0.9281893007170695\n",
      "Acc 0.474\n",
      "Epoch 0 - Training loss: 0.8283872519220624\n",
      "Acc 0.509\n",
      "Epoch 0 - Training loss: 0.6446093983948231\n",
      "Acc 0.523\n",
      "Epoch 0 - Training loss: 0.5949507185723633\n",
      "Acc 0.539\n",
      "Epoch 0 - Training loss: 0.5778817737146336\n",
      "Acc 0.559\n",
      "Epoch 0 - Training loss: 0.48229599506076837\n",
      "Acc 0.567\n",
      "Epoch 0 - Training loss: 0.42292896589558376\n",
      "Acc 0.578\n",
      "Epoch 0 - Training loss: 0.44855695727746936\n",
      "Acc 0.59\n",
      "Epoch 0 - Training loss: 0.3664057333288448\n",
      "Acc 0.601\n",
      "Epoch 0 - Training loss: 0.31710768677294254\n",
      "Acc 0.609\n",
      "Epoch 0 - Training loss: 0.2978630272023704\n",
      "Acc 0.619\n",
      "Epoch 0 - Training loss: 0.2629253196452434\n",
      "Acc 0.625\n",
      "Epoch 0 - Training loss: 0.2421572694182396\n",
      "Acc 0.636\n",
      "Epoch 0 - Training loss: 0.22529645910701498\n",
      "Acc 0.642\n",
      "Epoch 0 - Training loss: 0.18696948723798548\n",
      "Acc 0.647\n",
      "Epoch 0 - Training loss: 0.1518371813664479\n",
      "Acc 0.646\n",
      "Epoch 0 - Training loss: 0.1827942874475286\n",
      "Acc 0.661\n",
      "Epoch 0 - Training loss: 0.19536106038528184\n",
      "Acc 0.677\n",
      "Epoch 0 - Training loss: 0.16867144290177571\n",
      "Acc 0.678\n",
      "Epoch 0 - Training loss: 0.15853498462820426\n",
      "Acc 0.67\n",
      "Epoch 0 - Training loss: 0.12032320614283283\n",
      "Acc 0.673\n",
      "Epoch 0 - Training loss: 0.11708362389575033\n",
      "Acc 0.683\n",
      "Epoch 0 - Training loss: 0.10019794590771199\n",
      "Acc 0.688\n",
      "Epoch 0 - Training loss: 0.08019282376497155\n",
      "Acc 0.691\n",
      "Epoch 0 - Training loss: 0.10044649563024978\n",
      "Acc 0.702\n",
      "Epoch 0 - Training loss: 0.13551813125071166\n",
      "Acc 0.71\n",
      "Epoch 0 - Training loss: 0.1495551466046331\n",
      "Acc 0.713\n",
      "Epoch 0 - Training loss: 0.12509157868335025\n",
      "Acc 0.704\n",
      "Epoch 0 - Training loss: 0.09687207420015843\n",
      "Acc 0.718\n",
      "Epoch 0 - Training loss: 0.10889490223607504\n",
      "Acc 0.71\n",
      "Epoch 0 - Training loss: 0.11153519667988253\n",
      "Acc 0.722\n",
      "Epoch 0 - Training loss: 0.11262929333447987\n",
      "Acc 0.729\n",
      "Epoch 0 - Training loss: 0.10946041406908383\n",
      "Acc 0.731\n",
      "Epoch 0 - Training loss: 0.09903281556730888\n",
      "Acc 0.751\n",
      "Epoch 0 - Training loss: 0.10067534299277542\n",
      "Acc 0.745\n",
      "Epoch 0 - Training loss: 0.07756239242250255\n",
      "Acc 0.753\n",
      "Epoch 0 - Training loss: 0.07474957384188108\n",
      "Acc 0.746\n",
      "Epoch 0 - Training loss: 0.069898716006428\n",
      "Acc 0.751\n",
      "Epoch 0 - Training loss: 0.06486007287258319\n",
      "Acc 0.752\n",
      "Epoch 0 - Training loss: 0.04767953003801477\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.061389283803159826\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.07207262218947073\n",
      "Acc 0.767\n",
      "Epoch 0 - Training loss: 0.06718006701028736\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.06157642153786063\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.047561465391864705\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.05101137842625733\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.04236736409445056\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.035953133118649325\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.03535470341286454\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.03863199543012606\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.05056203065610062\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.05089422434548396\n",
      "Acc 0.785\n",
      "Epoch 0 - Training loss: 0.04285886087723506\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.03959838890692812\n",
      "Acc 0.796\n",
      "Epoch 0 - Training loss: 0.04551411418195592\n",
      "Acc 0.794\n",
      "Epoch 0 - Training loss: 0.049534485199461305\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.040355018035515444\n",
      "Acc 0.794\n",
      "Epoch 0 - Training loss: 0.03288474268421331\n",
      "Acc 0.789\n",
      "Epoch 0 - Training loss: 0.02989458139989079\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.031322111881308956\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.04501387332279031\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.046576107448640255\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.049797704479812334\n",
      "Acc 0.806\n",
      "Epoch 0 - Training loss: 0.04276255838148676\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.04820558590196109\n",
      "Acc 0.809\n",
      "Epoch 0 - Training loss: 0.04073133941244286\n",
      "Acc 0.813\n",
      "Epoch 0 - Training loss: 0.03552428580808772\n",
      "Acc 0.809\n",
      "Epoch 0 - Training loss: 0.03460081498160435\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.031094243489838216\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.026598383541923107\n",
      "Acc 0.806\n",
      "Epoch 0 - Training loss: 0.040846059055885314\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.0330010511530591\n",
      "Acc 0.811\n",
      "Episode 2 started\n",
      "Epoch 0 - Training loss: 2.8916191478744584\n",
      "Acc 0.166\n",
      "Epoch 0 - Training loss: 0.9864134668356421\n",
      "Acc 0.577\n",
      "Epoch 0 - Training loss: 0.6498983688439641\n",
      "Acc 0.668\n",
      "Epoch 0 - Training loss: 0.4564878081660823\n",
      "Acc 0.719\n",
      "Epoch 0 - Training loss: 0.34149932303575165\n",
      "Acc 0.741\n",
      "Epoch 0 - Training loss: 0.26805514118310003\n",
      "Acc 0.745\n",
      "Epoch 0 - Training loss: 0.21332239529628133\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.17616961570708334\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.14352508544184786\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.12131676635157967\n",
      "Acc 0.787\n",
      "Epoch 0 - Training loss: 0.10431987491889774\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.09078535111379747\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.0788160107528361\n",
      "Acc 0.806\n",
      "Epoch 0 - Training loss: 0.06971895813629939\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.06185039311609975\n",
      "Acc 0.81\n",
      "Epoch 0 - Training loss: 0.0553515682318966\n",
      "Acc 0.812\n",
      "Epoch 0 - Training loss: 0.05013638087708888\n",
      "Acc 0.819\n",
      "Epoch 0 - Training loss: 0.04516330120840859\n",
      "Acc 0.82\n",
      "Epoch 0 - Training loss: 0.041083548748910716\n",
      "Acc 0.823\n",
      "Epoch 0 - Training loss: 0.03782244600605166\n",
      "Acc 0.821\n",
      "Epoch 0 - Training loss: 0.03479669121054323\n",
      "Acc 0.821\n",
      "Epoch 0 - Training loss: 0.03261847513238134\n",
      "Acc 0.825\n",
      "Epoch 0 - Training loss: 0.030248653863253462\n",
      "Acc 0.823\n",
      "Epoch 0 - Training loss: 0.02812083700563424\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.026081064498566568\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.02487353199452364\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.02321723334892181\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.021920129467940166\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.02090563520881202\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.01972877245480764\n",
      "Acc 0.83\n",
      "Epoch 0 - Training loss: 0.01888258873039778\n",
      "Acc 0.832\n",
      "Epoch 0 - Training loss: 0.01763140844981098\n",
      "Acc 0.831\n",
      "Epoch 0 - Training loss: 0.01677520741470792\n",
      "Acc 0.83\n",
      "Epoch 0 - Training loss: 0.01600524229695038\n",
      "Acc 0.833\n",
      "Epoch 0 - Training loss: 0.015201630273436403\n",
      "Acc 0.832\n",
      "Epoch 0 - Training loss: 0.01450876531160255\n",
      "Acc 0.833\n",
      "Epoch 0 - Training loss: 0.013955569190093902\n",
      "Acc 0.833\n",
      "Epoch 0 - Training loss: 0.013516679966622697\n",
      "Acc 0.833\n",
      "Epoch 0 - Training loss: 0.01288324335402892\n",
      "Acc 0.834\n",
      "Epoch 0 - Training loss: 0.012412084380621052\n",
      "Acc 0.834\n",
      "Epoch 0 - Training loss: 0.011883378629846335\n",
      "Acc 0.836\n",
      "Epoch 0 - Training loss: 0.011475676490755921\n",
      "Acc 0.834\n",
      "Epoch 0 - Training loss: 0.011089844725268217\n",
      "Acc 0.836\n",
      "Epoch 0 - Training loss: 0.010674849596030373\n",
      "Acc 0.835\n",
      "Epoch 0 - Training loss: 0.01036281614983724\n",
      "Acc 0.834\n",
      "Epoch 0 - Training loss: 0.010211847730559009\n",
      "Acc 0.837\n",
      "Epoch 0 - Training loss: 0.0096777162462395\n",
      "Acc 0.836\n",
      "Epoch 0 - Training loss: 0.00939698650585136\n",
      "Acc 0.835\n",
      "Epoch 0 - Training loss: 0.009138730265660568\n",
      "Acc 0.837\n",
      "Epoch 0 - Training loss: 0.00881106670938093\n",
      "Acc 0.837\n",
      "Epoch 0 - Training loss: 0.008588399441880123\n",
      "Acc 0.839\n",
      "Epoch 0 - Training loss: 0.008218056605037152\n",
      "Acc 0.839\n",
      "Epoch 0 - Training loss: 0.007989398243371398\n",
      "Acc 0.839\n",
      "Epoch 0 - Training loss: 0.007796517544823601\n",
      "Acc 0.841\n",
      "Epoch 0 - Training loss: 0.007567642194028663\n",
      "Acc 0.839\n",
      "Epoch 0 - Training loss: 0.00739328954592546\n",
      "Acc 0.841\n",
      "Epoch 0 - Training loss: 0.007175339447262832\n",
      "Acc 0.839\n",
      "Epoch 0 - Training loss: 0.006956764318359395\n",
      "Acc 0.839\n",
      "Epoch 0 - Training loss: 0.006768423348392655\n",
      "Acc 0.841\n",
      "Epoch 0 - Training loss: 0.00668126929244816\n",
      "Acc 0.842\n",
      "Epoch 0 - Training loss: 0.00646830960965989\n",
      "Acc 0.842\n",
      "Epoch 0 - Training loss: 0.006264786780070841\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.006097709663287279\n",
      "Acc 0.842\n",
      "Epoch 0 - Training loss: 0.006025308228486103\n",
      "Acc 0.844\n",
      "Epoch 0 - Training loss: 0.005843371084644412\n",
      "Acc 0.844\n",
      "Epoch 0 - Training loss: 0.005669276729105056\n",
      "Acc 0.843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.0055691109911234025\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.005456203099657003\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.00530580824911473\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.00517362817260286\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.00502726187401431\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.004914949005333924\n",
      "Acc 0.843\n",
      "Epoch 0 - Training loss: 0.00492353061983724\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.004765787162162123\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.004655432134188294\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.004562424260310936\n",
      "Acc 0.844\n",
      "Epoch 0 - Training loss: 0.004509183179029198\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.004382258348252016\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.004282799856159185\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.004189135210657486\n",
      "Acc 0.844\n",
      "Epoch 0 - Training loss: 0.004128686549729619\n",
      "Acc 0.845\n",
      "Epoch 0 - Training loss: 0.00402754014475476\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.003974972442035744\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.00388046164556651\n",
      "Acc 0.847\n",
      "Epoch 0 - Training loss: 0.004351921814637337\n",
      "Acc 0.846\n",
      "Epoch 0 - Training loss: 0.004211160354209549\n",
      "Acc 0.847\n",
      "Epoch 0 - Training loss: 0.0047940011249509645\n",
      "Acc 0.847\n",
      "Epoch 0 - Training loss: 0.006815551720311582\n",
      "Acc 0.848\n",
      "Epoch 0 - Training loss: 0.006348459491043682\n",
      "Acc 0.85\n",
      "Epoch 0 - Training loss: 0.005620592975508166\n",
      "Acc 0.848\n",
      "Epoch 0 - Training loss: 0.010225613145566475\n",
      "Acc 0.847\n",
      "Epoch 0 - Training loss: 0.018692511636241096\n",
      "Acc 0.847\n",
      "Epoch 0 - Training loss: 0.027232124080551905\n",
      "Acc 0.852\n",
      "Episode 3 started\n",
      "Epoch 0 - Training loss: 13.14321231842041\n",
      "Acc 0.033\n",
      "Epoch 0 - Training loss: 12.025553226470947\n",
      "Acc 0.033\n",
      "Epoch 0 - Training loss: 8.506346543629965\n",
      "Acc 0.04\n",
      "Epoch 0 - Training loss: 6.037464916706085\n",
      "Acc 0.076\n",
      "Epoch 0 - Training loss: 3.791428470611572\n",
      "Acc 0.129\n",
      "Epoch 0 - Training loss: 2.3042854368686676\n",
      "Acc 0.226\n",
      "Epoch 0 - Training loss: 1.6782129194055284\n",
      "Acc 0.279\n",
      "Epoch 0 - Training loss: 1.4734695572406054\n",
      "Acc 0.303\n",
      "Epoch 0 - Training loss: 1.1375375820530786\n",
      "Acc 0.323\n",
      "Epoch 0 - Training loss: 1.0152911573648453\n",
      "Acc 0.343\n",
      "Epoch 0 - Training loss: 0.9530192498456348\n",
      "Acc 0.37\n",
      "Epoch 0 - Training loss: 0.7179096868882576\n",
      "Acc 0.408\n",
      "Epoch 0 - Training loss: 0.5619843447437654\n",
      "Acc 0.429\n",
      "Epoch 0 - Training loss: 0.5200722036617142\n",
      "Acc 0.441\n",
      "Epoch 0 - Training loss: 0.4671897155543168\n",
      "Acc 0.453\n",
      "Epoch 0 - Training loss: 0.5053497190820053\n",
      "Acc 0.481\n",
      "Epoch 0 - Training loss: 0.46226021022919345\n",
      "Acc 0.482\n",
      "Epoch 0 - Training loss: 0.483806269036399\n",
      "Acc 0.499\n",
      "Epoch 0 - Training loss: 0.4455744410423856\n",
      "Acc 0.507\n",
      "Epoch 0 - Training loss: 0.41104110367596147\n",
      "Acc 0.51\n",
      "Epoch 0 - Training loss: 0.37569535638959634\n",
      "Acc 0.526\n",
      "Epoch 0 - Training loss: 0.3738027768929235\n",
      "Acc 0.548\n",
      "Epoch 0 - Training loss: 0.3618398617305186\n",
      "Acc 0.55\n",
      "Epoch 0 - Training loss: 0.2682655412936583\n",
      "Acc 0.568\n",
      "Epoch 0 - Training loss: 0.21634855357930063\n",
      "Acc 0.577\n",
      "Epoch 0 - Training loss: 0.22006558891958916\n",
      "Acc 0.594\n",
      "Epoch 0 - Training loss: 0.19020563257099302\n",
      "Acc 0.601\n",
      "Epoch 0 - Training loss: 0.17365185503980943\n",
      "Acc 0.607\n",
      "Epoch 0 - Training loss: 0.15629130063963861\n",
      "Acc 0.622\n",
      "Epoch 0 - Training loss: 0.15749909107883772\n",
      "Acc 0.629\n",
      "Epoch 0 - Training loss: 0.1500022719584165\n",
      "Acc 0.631\n",
      "Epoch 0 - Training loss: 0.17046750741428696\n",
      "Acc 0.635\n",
      "Epoch 0 - Training loss: 0.20360835619045026\n",
      "Acc 0.638\n",
      "Epoch 0 - Training loss: 0.21839252232080875\n",
      "Acc 0.652\n",
      "Epoch 0 - Training loss: 0.20697415934077332\n",
      "Acc 0.649\n",
      "Epoch 0 - Training loss: 0.18686089741014358\n",
      "Acc 0.658\n",
      "Epoch 0 - Training loss: 0.1605196725891752\n",
      "Acc 0.656\n",
      "Epoch 0 - Training loss: 0.14691155373216852\n",
      "Acc 0.667\n",
      "Epoch 0 - Training loss: 0.1415135303321175\n",
      "Acc 0.673\n",
      "Epoch 0 - Training loss: 0.1258865636831615\n",
      "Acc 0.681\n",
      "Epoch 0 - Training loss: 0.11651535744483514\n",
      "Acc 0.688\n",
      "Epoch 0 - Training loss: 0.10883462269391332\n",
      "Acc 0.689\n",
      "Epoch 0 - Training loss: 0.11558254995534933\n",
      "Acc 0.704\n",
      "Epoch 0 - Training loss: 0.1001049604118717\n",
      "Acc 0.713\n",
      "Epoch 0 - Training loss: 0.08845151032631596\n",
      "Acc 0.711\n",
      "Epoch 0 - Training loss: 0.08376847480742625\n",
      "Acc 0.719\n",
      "Epoch 0 - Training loss: 0.08424946960022157\n",
      "Acc 0.723\n",
      "Epoch 0 - Training loss: 0.07848494281400538\n",
      "Acc 0.726\n",
      "Epoch 0 - Training loss: 0.07472922690022661\n",
      "Acc 0.729\n",
      "Epoch 0 - Training loss: 0.08193560699000954\n",
      "Acc 0.73\n",
      "Epoch 0 - Training loss: 0.10720677018695164\n",
      "Acc 0.735\n",
      "Epoch 0 - Training loss: 0.10931704106042162\n",
      "Acc 0.728\n",
      "Epoch 0 - Training loss: 0.10535634460173687\n",
      "Acc 0.723\n",
      "Epoch 0 - Training loss: 0.09678953796780358\n",
      "Acc 0.729\n",
      "Epoch 0 - Training loss: 0.09556799438909035\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.08782832401006349\n",
      "Acc 0.74\n",
      "Epoch 0 - Training loss: 0.0772576015432398\n",
      "Acc 0.736\n",
      "Epoch 0 - Training loss: 0.07731794524769267\n",
      "Acc 0.736\n",
      "Epoch 0 - Training loss: 0.0693366389832128\n",
      "Acc 0.74\n",
      "Epoch 0 - Training loss: 0.09920007068819056\n",
      "Acc 0.75\n",
      "Epoch 0 - Training loss: 0.08264691086837136\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.07179374711009703\n",
      "Acc 0.76\n",
      "Epoch 0 - Training loss: 0.05958024536200341\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.05155001648108737\n",
      "Acc 0.76\n",
      "Epoch 0 - Training loss: 0.06799964409303637\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.05669201241504528\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.07753980594405682\n",
      "Acc 0.767\n",
      "Epoch 0 - Training loss: 0.06632481167017591\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.06310484735835073\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.05368105380496543\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.050400242597212305\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.05059782114363366\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.05213894178311034\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.05062501988081872\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.05126114801173875\n",
      "Acc 0.79\n",
      "Epoch 0 - Training loss: 0.04781375523108229\n",
      "Acc 0.785\n",
      "Epoch 0 - Training loss: 0.05139182633100465\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.05154639316326491\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.04983739557018786\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.054800582897769345\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.04731132289964956\n",
      "Acc 0.801\n",
      "Epoch 0 - Training loss: 0.04109984277622017\n",
      "Acc 0.805\n",
      "Epoch 0 - Training loss: 0.034473936317533434\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.03924325484296701\n",
      "Acc 0.802\n",
      "Epoch 0 - Training loss: 0.03150540279542945\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.02580785325479282\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.041172040624734035\n",
      "Acc 0.802\n",
      "Epoch 0 - Training loss: 0.03438931579669522\n",
      "Acc 0.805\n",
      "Epoch 0 - Training loss: 0.038705478233005325\n",
      "Acc 0.808\n",
      "Epoch 0 - Training loss: 0.031245487814562187\n",
      "Acc 0.812\n",
      "Epoch 0 - Training loss: 0.028092849658255894\n",
      "Acc 0.813\n",
      "Epoch 0 - Training loss: 0.02496427530549345\n",
      "Acc 0.815\n",
      "Epoch 0 - Training loss: 0.03461700111849143\n",
      "Acc 0.816\n",
      "Episode 4 started\n",
      "Epoch 0 - Training loss: 10.943329811096191\n",
      "Acc 0.098\n",
      "Epoch 0 - Training loss: 11.857434034347534\n",
      "Acc 0.095\n",
      "Epoch 0 - Training loss: 7.987161954243978\n",
      "Acc 0.104\n",
      "Epoch 0 - Training loss: 4.854223281145096\n",
      "Acc 0.113\n",
      "Epoch 0 - Training loss: 3.4233291029930113\n",
      "Acc 0.151\n",
      "Epoch 0 - Training loss: 2.2584762225548425\n",
      "Acc 0.218\n",
      "Epoch 0 - Training loss: 1.4496955083949226\n",
      "Acc 0.259\n",
      "Epoch 0 - Training loss: 1.1474211690947413\n",
      "Acc 0.303\n",
      "Epoch 0 - Training loss: 0.9063988170690007\n",
      "Acc 0.332\n",
      "Epoch 0 - Training loss: 0.7171044902876019\n",
      "Acc 0.371\n",
      "Epoch 0 - Training loss: 0.6217772647399794\n",
      "Acc 0.398\n",
      "Epoch 0 - Training loss: 0.44724215008318424\n",
      "Acc 0.421\n",
      "Epoch 0 - Training loss: 0.5432459390364014\n",
      "Acc 0.444\n",
      "Epoch 0 - Training loss: 0.4183257570756333\n",
      "Acc 0.469\n",
      "Epoch 0 - Training loss: 0.34718684225032725\n",
      "Acc 0.489\n",
      "Epoch 0 - Training loss: 0.31688411510549486\n",
      "Acc 0.502\n",
      "Epoch 0 - Training loss: 0.3050753330154454\n",
      "Acc 0.522\n",
      "Epoch 0 - Training loss: 0.29908982581562465\n",
      "Acc 0.53\n",
      "Epoch 0 - Training loss: 0.3696396609670238\n",
      "Acc 0.543\n",
      "Epoch 0 - Training loss: 0.285792220197618\n",
      "Acc 0.552\n",
      "Epoch 0 - Training loss: 0.2790741423544075\n",
      "Acc 0.56\n",
      "Epoch 0 - Training loss: 0.2706937863364477\n",
      "Acc 0.572\n",
      "Epoch 0 - Training loss: 0.23079978825483957\n",
      "Acc 0.586\n",
      "Epoch 0 - Training loss: 0.2145301645117191\n",
      "Acc 0.59\n",
      "Epoch 0 - Training loss: 0.2289958268404007\n",
      "Acc 0.597\n",
      "Epoch 0 - Training loss: 0.19167458043935207\n",
      "Acc 0.614\n",
      "Epoch 0 - Training loss: 0.1419016215087915\n",
      "Acc 0.623\n",
      "Epoch 0 - Training loss: 0.11692724115813949\n",
      "Acc 0.626\n",
      "Epoch 0 - Training loss: 0.12672666330211635\n",
      "Acc 0.639\n",
      "Epoch 0 - Training loss: 0.13759198398329317\n",
      "Acc 0.649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.12988266221157485\n",
      "Acc 0.656\n",
      "Epoch 0 - Training loss: 0.09328959511913126\n",
      "Acc 0.671\n",
      "Epoch 0 - Training loss: 0.0727057622773855\n",
      "Acc 0.676\n",
      "Epoch 0 - Training loss: 0.09553789312485605\n",
      "Acc 0.667\n",
      "Epoch 0 - Training loss: 0.12112596593298285\n",
      "Acc 0.677\n",
      "Epoch 0 - Training loss: 0.1298252023261739\n",
      "Acc 0.678\n",
      "Epoch 0 - Training loss: 0.10814676158337828\n",
      "Acc 0.701\n",
      "Epoch 0 - Training loss: 0.10058898659190163\n",
      "Acc 0.707\n",
      "Epoch 0 - Training loss: 0.07248938423939623\n",
      "Acc 0.706\n",
      "Epoch 0 - Training loss: 0.0937031322187977\n",
      "Acc 0.706\n",
      "Epoch 0 - Training loss: 0.08924281145105274\n",
      "Acc 0.704\n",
      "Epoch 0 - Training loss: 0.07158516933088235\n",
      "Acc 0.717\n",
      "Epoch 0 - Training loss: 0.06144044606751481\n",
      "Acc 0.707\n",
      "Epoch 0 - Training loss: 0.07432387099701869\n",
      "Acc 0.723\n",
      "Epoch 0 - Training loss: 0.06619941940168954\n",
      "Acc 0.725\n",
      "Epoch 0 - Training loss: 0.10167007788297032\n",
      "Acc 0.732\n",
      "Epoch 0 - Training loss: 0.08626681005463321\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.0864190070466672\n",
      "Acc 0.741\n",
      "Epoch 0 - Training loss: 0.07886168087965675\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.08801987426122651\n",
      "Acc 0.746\n",
      "Epoch 0 - Training loss: 0.06330371006647599\n",
      "Acc 0.75\n",
      "Epoch 0 - Training loss: 0.055404088358045556\n",
      "Acc 0.76\n",
      "Epoch 0 - Training loss: 0.04616030865076508\n",
      "Acc 0.76\n",
      "Epoch 0 - Training loss: 0.05308939293199391\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.047830697724764996\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.039975287620628866\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.07247573435380585\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.05570774189254735\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.04972475923605705\n",
      "Acc 0.767\n",
      "Epoch 0 - Training loss: 0.04579842347593512\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.03431884309355185\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.03534566348366019\n",
      "Acc 0.776\n",
      "Epoch 0 - Training loss: 0.04579140159203523\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.05076650646833514\n",
      "Acc 0.782\n",
      "Epoch 0 - Training loss: 0.04670420144214474\n",
      "Acc 0.782\n",
      "Epoch 0 - Training loss: 0.05463408848740845\n",
      "Acc 0.785\n",
      "Epoch 0 - Training loss: 0.04643030565556959\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.07098834213042317\n",
      "Acc 0.784\n",
      "Epoch 0 - Training loss: 0.05969224810310086\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.044758393492416615\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.03430590861555661\n",
      "Acc 0.789\n",
      "Epoch 0 - Training loss: 0.034410073229284736\n",
      "Acc 0.785\n",
      "Epoch 0 - Training loss: 0.03430296916973918\n",
      "Acc 0.79\n",
      "Epoch 0 - Training loss: 0.041778597254715104\n",
      "Acc 0.789\n",
      "Epoch 0 - Training loss: 0.03180937215142573\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.03740234625920972\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.03704243515893687\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.04264878954033217\n",
      "Acc 0.796\n",
      "Epoch 0 - Training loss: 0.03283010938137039\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.039568347849308336\n",
      "Acc 0.802\n",
      "Epoch 0 - Training loss: 0.046082960046651016\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.04127537824725306\n",
      "Acc 0.801\n",
      "Epoch 0 - Training loss: 0.04444928336188131\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.04744831446164642\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.03683242833358236\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.03444931641637976\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.035578114916639064\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.03189725353513495\n",
      "Acc 0.81\n",
      "Epoch 0 - Training loss: 0.03697099064823745\n",
      "Acc 0.815\n",
      "Epoch 0 - Training loss: 0.031221371502761032\n",
      "Acc 0.814\n",
      "Epoch 0 - Training loss: 0.030111807633247636\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.028527718978179822\n",
      "Acc 0.816\n",
      "Epoch 0 - Training loss: 0.022925024270392974\n",
      "Acc 0.816\n",
      "Episode 5 started\n",
      "Epoch 0 - Training loss: 13.102154731750488\n",
      "Acc 0.134\n",
      "Epoch 0 - Training loss: 11.036671757698059\n",
      "Acc 0.136\n",
      "Epoch 0 - Training loss: 7.495836893717448\n",
      "Acc 0.133\n",
      "Epoch 0 - Training loss: 4.600887253880501\n",
      "Acc 0.139\n",
      "Epoch 0 - Training loss: 2.996698886156082\n",
      "Acc 0.186\n",
      "Epoch 0 - Training loss: 2.412607654929161\n",
      "Acc 0.251\n",
      "Epoch 0 - Training loss: 2.113582623789885\n",
      "Acc 0.287\n",
      "Epoch 0 - Training loss: 1.6735518649220467\n",
      "Acc 0.327\n",
      "Epoch 0 - Training loss: 1.296198916931947\n",
      "Acc 0.365\n",
      "Epoch 0 - Training loss: 1.0649021431803702\n",
      "Acc 0.392\n",
      "Epoch 0 - Training loss: 0.781371795995669\n",
      "Acc 0.418\n",
      "Epoch 0 - Training loss: 0.7319270597460369\n",
      "Acc 0.444\n",
      "Epoch 0 - Training loss: 0.6585587412118912\n",
      "Acc 0.453\n",
      "Epoch 0 - Training loss: 0.5656180973829967\n",
      "Acc 0.468\n",
      "Epoch 0 - Training loss: 0.5340328982720772\n",
      "Acc 0.492\n",
      "Epoch 0 - Training loss: 0.4499507946893573\n",
      "Acc 0.507\n",
      "Epoch 0 - Training loss: 0.4142243159003556\n",
      "Acc 0.52\n",
      "Epoch 0 - Training loss: 0.43095796674283016\n",
      "Acc 0.523\n",
      "Epoch 0 - Training loss: 0.37630681169072266\n",
      "Acc 0.545\n",
      "Epoch 0 - Training loss: 0.33665723274461923\n",
      "Acc 0.556\n",
      "Epoch 0 - Training loss: 0.3292256930754298\n",
      "Acc 0.575\n",
      "Epoch 0 - Training loss: 0.2829191765366969\n",
      "Acc 0.586\n",
      "Epoch 0 - Training loss: 0.2863655690022785\n",
      "Acc 0.597\n",
      "Epoch 0 - Training loss: 0.2093850790988654\n",
      "Acc 0.597\n",
      "Epoch 0 - Training loss: 0.19439431613311173\n",
      "Acc 0.603\n",
      "Epoch 0 - Training loss: 0.21286418077607566\n",
      "Acc 0.615\n",
      "Epoch 0 - Training loss: 0.21027481074755391\n",
      "Acc 0.624\n",
      "Epoch 0 - Training loss: 0.1543538383224846\n",
      "Acc 0.611\n",
      "Epoch 0 - Training loss: 0.18355153937791957\n",
      "Acc 0.63\n",
      "Epoch 0 - Training loss: 0.16815855260938406\n",
      "Acc 0.644\n",
      "Epoch 0 - Training loss: 0.14414840277224297\n",
      "Acc 0.644\n",
      "Epoch 0 - Training loss: 0.1437306528314366\n",
      "Acc 0.646\n",
      "Epoch 0 - Training loss: 0.14347160869334458\n",
      "Acc 0.651\n",
      "Epoch 0 - Training loss: 0.10528497586903326\n",
      "Acc 0.657\n",
      "Epoch 0 - Training loss: 0.0853504547816036\n",
      "Acc 0.668\n",
      "Epoch 0 - Training loss: 0.08447756604032798\n",
      "Acc 0.672\n",
      "Epoch 0 - Training loss: 0.07263503974693752\n",
      "Acc 0.677\n",
      "Epoch 0 - Training loss: 0.09293017641788251\n",
      "Acc 0.681\n",
      "Epoch 0 - Training loss: 0.07327205977904108\n",
      "Acc 0.684\n",
      "Epoch 0 - Training loss: 0.07006554312829394\n",
      "Acc 0.689\n",
      "Epoch 0 - Training loss: 0.07852285014885682\n",
      "Acc 0.699\n",
      "Epoch 0 - Training loss: 0.06509081473562955\n",
      "Acc 0.696\n",
      "Epoch 0 - Training loss: 0.07464571143662947\n",
      "Acc 0.697\n",
      "Epoch 0 - Training loss: 0.06102425632045858\n",
      "Acc 0.705\n",
      "Epoch 0 - Training loss: 0.04704004059054164\n",
      "Acc 0.705\n",
      "Epoch 0 - Training loss: 0.05004503163913994\n",
      "Acc 0.709\n",
      "Epoch 0 - Training loss: 0.04792683608483206\n",
      "Acc 0.712\n",
      "Epoch 0 - Training loss: 0.08232359918717218\n",
      "Acc 0.718\n",
      "Epoch 0 - Training loss: 0.06387686137851252\n",
      "Acc 0.717\n",
      "Epoch 0 - Training loss: 0.07004583103582263\n",
      "Acc 0.72\n",
      "Epoch 0 - Training loss: 0.06669042054472454\n",
      "Acc 0.718\n",
      "Epoch 0 - Training loss: 0.06729074232638456\n",
      "Acc 0.73\n",
      "Epoch 0 - Training loss: 0.05654366010742894\n",
      "Acc 0.732\n",
      "Epoch 0 - Training loss: 0.07055296597536653\n",
      "Acc 0.741\n",
      "Epoch 0 - Training loss: 0.07534905410668051\n",
      "Acc 0.735\n",
      "Epoch 0 - Training loss: 0.06934136212553962\n",
      "Acc 0.736\n",
      "Epoch 0 - Training loss: 0.05899535035694083\n",
      "Acc 0.74\n",
      "Epoch 0 - Training loss: 0.04650317104673816\n",
      "Acc 0.748\n",
      "Epoch 0 - Training loss: 0.046606078737028635\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.03611976664396934\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.038677731979271913\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.05393028763712444\n",
      "Acc 0.755\n",
      "Epoch 0 - Training loss: 0.04708528160930626\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.03453036730297754\n",
      "Acc 0.76\n",
      "Epoch 0 - Training loss: 0.039348125034638755\n",
      "Acc 0.765\n",
      "Epoch 0 - Training loss: 0.04179229359380605\n",
      "Acc 0.768\n",
      "Epoch 0 - Training loss: 0.04508460562245281\n",
      "Acc 0.764\n",
      "Epoch 0 - Training loss: 0.03106223046865232\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.042814973382400756\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.028579031875311297\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.045254259587521335\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.045071620329003456\n",
      "Acc 0.771\n",
      "Epoch 0 - Training loss: 0.04036421802258502\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.04274505525127343\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.03535626507558239\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.037287422346324955\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.03424115101209624\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.036700375143808715\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.024373580242803356\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.04557575380949856\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.0460376329044524\n",
      "Acc 0.794\n",
      "Epoch 0 - Training loss: 0.04358819943551746\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.030240151773544215\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.03965664878965721\n",
      "Acc 0.794\n",
      "Epoch 0 - Training loss: 0.04322560003253303\n",
      "Acc 0.8\n",
      "Epoch 0 - Training loss: 0.032275657859044744\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.034665970821465476\n",
      "Acc 0.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.03821429338987317\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.03167023020371366\n",
      "Acc 0.795\n",
      "Epoch 0 - Training loss: 0.030227575930702084\n",
      "Acc 0.802\n",
      "Epoch 0 - Training loss: 0.038697431849349026\n",
      "Acc 0.805\n",
      "Epoch 0 - Training loss: 0.036500569382753936\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.033333590905033725\n",
      "Acc 0.801\n",
      "Episode 6 started\n",
      "Epoch 0 - Training loss: 9.177235126495361\n",
      "Acc 0.116\n",
      "Epoch 0 - Training loss: 8.19506049156189\n",
      "Acc 0.123\n",
      "Epoch 0 - Training loss: 6.009455839792888\n",
      "Acc 0.132\n",
      "Epoch 0 - Training loss: 3.9757786840200424\n",
      "Acc 0.169\n",
      "Epoch 0 - Training loss: 2.799916994571686\n",
      "Acc 0.225\n",
      "Epoch 0 - Training loss: 2.069724455475807\n",
      "Acc 0.278\n",
      "Epoch 0 - Training loss: 1.6163665098803384\n",
      "Acc 0.314\n",
      "Epoch 0 - Training loss: 1.2384975859895349\n",
      "Acc 0.346\n",
      "Epoch 0 - Training loss: 1.0289650426970587\n",
      "Acc 0.383\n",
      "Epoch 0 - Training loss: 0.8425578653812409\n",
      "Acc 0.408\n",
      "Epoch 0 - Training loss: 0.6808893545107408\n",
      "Acc 0.444\n",
      "Epoch 0 - Training loss: 0.5422363799686233\n",
      "Acc 0.465\n",
      "Epoch 0 - Training loss: 0.5495997340633318\n",
      "Acc 0.501\n",
      "Epoch 0 - Training loss: 0.6083009604896817\n",
      "Acc 0.519\n",
      "Epoch 0 - Training loss: 0.5146813817322254\n",
      "Acc 0.54\n",
      "Epoch 0 - Training loss: 0.43211989058181643\n",
      "Acc 0.568\n",
      "Epoch 0 - Training loss: 0.3902263212401201\n",
      "Acc 0.577\n",
      "Epoch 0 - Training loss: 0.33749158018165165\n",
      "Acc 0.58\n",
      "Epoch 0 - Training loss: 0.2533372503362204\n",
      "Acc 0.582\n",
      "Epoch 0 - Training loss: 0.1929943879134953\n",
      "Acc 0.598\n",
      "Epoch 0 - Training loss: 0.23752723862638786\n",
      "Acc 0.607\n",
      "Epoch 0 - Training loss: 0.18686823987148024\n",
      "Acc 0.615\n",
      "Epoch 0 - Training loss: 0.18741872744715732\n",
      "Acc 0.624\n",
      "Epoch 0 - Training loss: 0.1421417604239347\n",
      "Acc 0.642\n",
      "Epoch 0 - Training loss: 0.13298336897045374\n",
      "Acc 0.644\n",
      "Epoch 0 - Training loss: 0.13332792045548558\n",
      "Acc 0.659\n",
      "Epoch 0 - Training loss: 0.10837674580721392\n",
      "Acc 0.659\n",
      "Epoch 0 - Training loss: 0.11138060952570024\n",
      "Acc 0.667\n",
      "Epoch 0 - Training loss: 0.10882745150894184\n",
      "Acc 0.67\n",
      "Epoch 0 - Training loss: 0.13751482426499326\n",
      "Acc 0.669\n",
      "Epoch 0 - Training loss: 0.137533193889765\n",
      "Acc 0.673\n",
      "Epoch 0 - Training loss: 0.12015195967251202\n",
      "Acc 0.679\n",
      "Epoch 0 - Training loss: 0.10198308316510961\n",
      "Acc 0.68\n",
      "Epoch 0 - Training loss: 0.10633144426323912\n",
      "Acc 0.694\n",
      "Epoch 0 - Training loss: 0.08859955597269747\n",
      "Acc 0.689\n",
      "Epoch 0 - Training loss: 0.0933472707175598\n",
      "Acc 0.698\n",
      "Epoch 0 - Training loss: 0.11437171846754043\n",
      "Acc 0.701\n",
      "Epoch 0 - Training loss: 0.09220671946624\n",
      "Acc 0.705\n",
      "Epoch 0 - Training loss: 0.11056612701250766\n",
      "Acc 0.706\n",
      "Epoch 0 - Training loss: 0.12112320179003291\n",
      "Acc 0.712\n",
      "Epoch 0 - Training loss: 0.1643056945418144\n",
      "Acc 0.714\n",
      "Epoch 0 - Training loss: 0.13816182104416103\n",
      "Acc 0.719\n",
      "Epoch 0 - Training loss: 0.11970502740519424\n",
      "Acc 0.715\n",
      "Epoch 0 - Training loss: 0.09913981334036427\n",
      "Acc 0.723\n",
      "Epoch 0 - Training loss: 0.09688477154510716\n",
      "Acc 0.719\n",
      "Epoch 0 - Training loss: 0.0819168931111167\n",
      "Acc 0.727\n",
      "Epoch 0 - Training loss: 0.06623649313312738\n",
      "Acc 0.732\n",
      "Epoch 0 - Training loss: 0.10549903121379127\n",
      "Acc 0.729\n",
      "Epoch 0 - Training loss: 0.1031400399788625\n",
      "Acc 0.731\n",
      "Epoch 0 - Training loss: 0.0983635068591684\n",
      "Acc 0.738\n",
      "Epoch 0 - Training loss: 0.10353398875689462\n",
      "Acc 0.737\n",
      "Epoch 0 - Training loss: 0.07898655560095974\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.08782370620899184\n",
      "Acc 0.735\n",
      "Epoch 0 - Training loss: 0.0738251676041357\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.07350905499929054\n",
      "Acc 0.738\n",
      "Epoch 0 - Training loss: 0.06766847466393042\n",
      "Acc 0.745\n",
      "Epoch 0 - Training loss: 0.07504059818906203\n",
      "Acc 0.747\n",
      "Epoch 0 - Training loss: 0.08125250010373841\n",
      "Acc 0.751\n",
      "Epoch 0 - Training loss: 0.06386110135275176\n",
      "Acc 0.753\n",
      "Epoch 0 - Training loss: 0.06512951015320141\n",
      "Acc 0.75\n",
      "Epoch 0 - Training loss: 0.07251267912835799\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.07335734917339118\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.08840671459666549\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.08428930677382596\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.061834739674276745\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.06544630997108691\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.05121469868645906\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.03867358796004042\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.035799215977549204\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.027545247345863444\n",
      "Acc 0.782\n",
      "Epoch 0 - Training loss: 0.03705253278833329\n",
      "Acc 0.787\n",
      "Epoch 0 - Training loss: 0.06489803838849184\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.06613457202193748\n",
      "Acc 0.787\n",
      "Epoch 0 - Training loss: 0.062431441194840356\n",
      "Acc 0.79\n",
      "Epoch 0 - Training loss: 0.06284209699253551\n",
      "Acc 0.795\n",
      "Epoch 0 - Training loss: 0.06612299956989438\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.07330777668875117\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.06584180820582566\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.06497394710210376\n",
      "Acc 0.805\n",
      "Epoch 0 - Training loss: 0.06707900603796588\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.05959536660251346\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.0560264107393223\n",
      "Acc 0.808\n",
      "Epoch 0 - Training loss: 0.059842656633370635\n",
      "Acc 0.809\n",
      "Epoch 0 - Training loss: 0.05364397997341473\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.04429529298692668\n",
      "Acc 0.815\n",
      "Epoch 0 - Training loss: 0.03473789250304879\n",
      "Acc 0.814\n",
      "Epoch 0 - Training loss: 0.028863697871766535\n",
      "Acc 0.819\n",
      "Epoch 0 - Training loss: 0.025420579697641766\n",
      "Acc 0.816\n",
      "Epoch 0 - Training loss: 0.021396644568800655\n",
      "Acc 0.824\n",
      "Epoch 0 - Training loss: 0.02752872688514698\n",
      "Acc 0.827\n",
      "Epoch 0 - Training loss: 0.03647707698909581\n",
      "Acc 0.823\n",
      "Epoch 0 - Training loss: 0.04292648092941027\n",
      "Acc 0.83\n",
      "Epoch 0 - Training loss: 0.04509299124041992\n",
      "Acc 0.827\n",
      "Episode 7 started\n",
      "Epoch 0 - Training loss: 10.27929973602295\n",
      "Acc 0.075\n",
      "Epoch 0 - Training loss: 9.242376446723938\n",
      "Acc 0.074\n",
      "Epoch 0 - Training loss: 6.844710111618042\n",
      "Acc 0.074\n",
      "Epoch 0 - Training loss: 3.950137883424759\n",
      "Acc 0.106\n",
      "Epoch 0 - Training loss: 3.4299890995025635\n",
      "Acc 0.152\n",
      "Epoch 0 - Training loss: 2.312193642059962\n",
      "Acc 0.23\n",
      "Epoch 0 - Training loss: 1.6157482032264983\n",
      "Acc 0.246\n",
      "Epoch 0 - Training loss: 1.1725813299417496\n",
      "Acc 0.289\n",
      "Epoch 0 - Training loss: 1.0552162577708561\n",
      "Acc 0.316\n",
      "Epoch 0 - Training loss: 0.8817432604730129\n",
      "Acc 0.353\n",
      "Epoch 0 - Training loss: 0.7043857858939604\n",
      "Acc 0.385\n",
      "Epoch 0 - Training loss: 0.6476747511575619\n",
      "Acc 0.404\n",
      "Epoch 0 - Training loss: 0.573855848553089\n",
      "Acc 0.431\n",
      "Epoch 0 - Training loss: 0.5755348996525365\n",
      "Acc 0.454\n",
      "Epoch 0 - Training loss: 0.516986967002352\n",
      "Acc 0.473\n",
      "Epoch 0 - Training loss: 0.47139610385056585\n",
      "Acc 0.487\n",
      "Epoch 0 - Training loss: 0.4314693606951657\n",
      "Acc 0.507\n",
      "Epoch 0 - Training loss: 0.3958564434821407\n",
      "Acc 0.53\n",
      "Epoch 0 - Training loss: 0.3344358514602247\n",
      "Acc 0.54\n",
      "Epoch 0 - Training loss: 0.2840302612166852\n",
      "Acc 0.552\n",
      "Epoch 0 - Training loss: 0.28444401423136395\n",
      "Acc 0.563\n",
      "Epoch 0 - Training loss: 0.22966272921555422\n",
      "Acc 0.585\n",
      "Epoch 0 - Training loss: 0.19484896957874298\n",
      "Acc 0.588\n",
      "Epoch 0 - Training loss: 0.18935869249980897\n",
      "Acc 0.609\n",
      "Epoch 0 - Training loss: 0.19714298283681272\n",
      "Acc 0.617\n",
      "Epoch 0 - Training loss: 0.19924369887807047\n",
      "Acc 0.629\n",
      "Epoch 0 - Training loss: 0.18957093658132684\n",
      "Acc 0.636\n",
      "Epoch 0 - Training loss: 0.2154415989727048\n",
      "Acc 0.643\n",
      "Epoch 0 - Training loss: 0.17313617570646878\n",
      "Acc 0.652\n",
      "Epoch 0 - Training loss: 0.14991130599131186\n",
      "Acc 0.664\n",
      "Epoch 0 - Training loss: 0.16373106173329777\n",
      "Acc 0.673\n",
      "Epoch 0 - Training loss: 0.14149720370187424\n",
      "Acc 0.676\n",
      "Epoch 0 - Training loss: 0.14004053075967188\n",
      "Acc 0.687\n",
      "Epoch 0 - Training loss: 0.12999947879956486\n",
      "Acc 0.692\n",
      "Epoch 0 - Training loss: 0.11891632605610149\n",
      "Acc 0.691\n",
      "Epoch 0 - Training loss: 0.12381045951042324\n",
      "Acc 0.711\n",
      "Epoch 0 - Training loss: 0.1025752203941748\n",
      "Acc 0.717\n",
      "Epoch 0 - Training loss: 0.11422393749173927\n",
      "Acc 0.718\n",
      "Epoch 0 - Training loss: 0.15762173951579592\n",
      "Acc 0.719\n",
      "Epoch 0 - Training loss: 0.14779292937600985\n",
      "Acc 0.725\n",
      "Epoch 0 - Training loss: 0.12343941053103019\n",
      "Acc 0.726\n",
      "Epoch 0 - Training loss: 0.13032153693382584\n",
      "Acc 0.739\n",
      "Epoch 0 - Training loss: 0.11545675991070566\n",
      "Acc 0.737\n",
      "Epoch 0 - Training loss: 0.09716539219847288\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.07908827318137304\n",
      "Acc 0.753\n",
      "Epoch 0 - Training loss: 0.11394799221053963\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.10255795926786959\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.1243792041229123\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.09896890976352199\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.08904062340967357\n",
      "Acc 0.771\n",
      "Epoch 0 - Training loss: 0.08886762472170898\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.08077014617335337\n",
      "Acc 0.777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.08540839135409596\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.08121890386273325\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.08884869673797353\n",
      "Acc 0.785\n",
      "Epoch 0 - Training loss: 0.08109782421420927\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.07731937758395807\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.07016924804614472\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.06008161128680143\n",
      "Acc 0.784\n",
      "Epoch 0 - Training loss: 0.0678739440588591\n",
      "Acc 0.784\n",
      "Epoch 0 - Training loss: 0.07407372971790553\n",
      "Acc 0.782\n",
      "Epoch 0 - Training loss: 0.05968116483023961\n",
      "Acc 0.787\n",
      "Epoch 0 - Training loss: 0.05752157914038333\n",
      "Acc 0.79\n",
      "Epoch 0 - Training loss: 0.07185040799595299\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.06970820580161391\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.054737982574310576\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.057602543265683884\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.053688392942764465\n",
      "Acc 0.797\n",
      "Epoch 0 - Training loss: 0.052171321474420634\n",
      "Acc 0.798\n",
      "Epoch 0 - Training loss: 0.04635170839776817\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.04362146407578954\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.05103620590105291\n",
      "Acc 0.806\n",
      "Epoch 0 - Training loss: 0.057546710267970785\n",
      "Acc 0.813\n",
      "Epoch 0 - Training loss: 0.07055599855123768\n",
      "Acc 0.809\n",
      "Epoch 0 - Training loss: 0.072246388780574\n",
      "Acc 0.817\n",
      "Epoch 0 - Training loss: 0.058149626853264635\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.054866610359107205\n",
      "Acc 0.819\n",
      "Epoch 0 - Training loss: 0.05954804431903773\n",
      "Acc 0.818\n",
      "Epoch 0 - Training loss: 0.0450579965898997\n",
      "Acc 0.823\n",
      "Epoch 0 - Training loss: 0.050485558502259664\n",
      "Acc 0.823\n",
      "Epoch 0 - Training loss: 0.04403601933612394\n",
      "Acc 0.823\n",
      "Epoch 0 - Training loss: 0.04430079729780385\n",
      "Acc 0.825\n",
      "Epoch 0 - Training loss: 0.05156552977123612\n",
      "Acc 0.826\n",
      "Epoch 0 - Training loss: 0.03990126957380978\n",
      "Acc 0.825\n",
      "Epoch 0 - Training loss: 0.04434187418425127\n",
      "Acc 0.832\n",
      "Epoch 0 - Training loss: 0.03695515056173209\n",
      "Acc 0.832\n",
      "Epoch 0 - Training loss: 0.03302558928834483\n",
      "Acc 0.828\n",
      "Epoch 0 - Training loss: 0.033513875645357155\n",
      "Acc 0.828\n",
      "Epoch 0 - Training loss: 0.039293301832882474\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.03620677730715316\n",
      "Acc 0.826\n",
      "Epoch 0 - Training loss: 0.04657918791725443\n",
      "Acc 0.831\n",
      "Epoch 0 - Training loss: 0.04487175014820304\n",
      "Acc 0.831\n",
      "Epoch 0 - Training loss: 0.04454752251362523\n",
      "Acc 0.837\n",
      "Episode 8 started\n",
      "Epoch 0 - Training loss: 11.559630870819092\n",
      "Acc 0.053\n",
      "Epoch 0 - Training loss: 10.184087872505188\n",
      "Acc 0.054\n",
      "Epoch 0 - Training loss: 7.735299547513326\n",
      "Acc 0.061\n",
      "Epoch 0 - Training loss: 5.921684592962265\n",
      "Acc 0.095\n",
      "Epoch 0 - Training loss: 4.07395521402359\n",
      "Acc 0.145\n",
      "Epoch 0 - Training loss: 3.058926691611608\n",
      "Acc 0.177\n",
      "Epoch 0 - Training loss: 2.5042183186326707\n",
      "Acc 0.202\n",
      "Epoch 0 - Training loss: 1.733126774430275\n",
      "Acc 0.233\n",
      "Epoch 0 - Training loss: 1.257929601603084\n",
      "Acc 0.276\n",
      "Epoch 0 - Training loss: 1.1885632663965224\n",
      "Acc 0.319\n",
      "Epoch 0 - Training loss: 0.9954601702365008\n",
      "Acc 0.344\n",
      "Epoch 0 - Training loss: 0.8565982412546873\n",
      "Acc 0.372\n",
      "Epoch 0 - Training loss: 0.7700757143589166\n",
      "Acc 0.41\n",
      "Epoch 0 - Training loss: 0.5619406875755105\n",
      "Acc 0.426\n",
      "Epoch 0 - Training loss: 0.5788522480676571\n",
      "Acc 0.441\n",
      "Epoch 0 - Training loss: 0.5781185401137918\n",
      "Acc 0.466\n",
      "Epoch 0 - Training loss: 0.5016527603216031\n",
      "Acc 0.491\n",
      "Epoch 0 - Training loss: 0.42933896659976906\n",
      "Acc 0.502\n",
      "Epoch 0 - Training loss: 0.41542363529534715\n",
      "Acc 0.518\n",
      "Epoch 0 - Training loss: 0.3843453194014728\n",
      "Acc 0.531\n",
      "Epoch 0 - Training loss: 0.3877755782256524\n",
      "Acc 0.537\n",
      "Epoch 0 - Training loss: 0.3713276647877964\n",
      "Acc 0.558\n",
      "Epoch 0 - Training loss: 0.3664443308649504\n",
      "Acc 0.565\n",
      "Epoch 0 - Training loss: 0.3288665311411023\n",
      "Acc 0.574\n",
      "Epoch 0 - Training loss: 0.2919664878025651\n",
      "Acc 0.588\n",
      "Epoch 0 - Training loss: 0.25654368383738285\n",
      "Acc 0.586\n",
      "Epoch 0 - Training loss: 0.23699527354566036\n",
      "Acc 0.596\n",
      "Epoch 0 - Training loss: 0.1977006451093725\n",
      "Acc 0.604\n",
      "Epoch 0 - Training loss: 0.17409123955615635\n",
      "Acc 0.607\n",
      "Epoch 0 - Training loss: 0.22050253671283523\n",
      "Acc 0.623\n",
      "Epoch 0 - Training loss: 0.24353893777926364\n",
      "Acc 0.624\n",
      "Epoch 0 - Training loss: 0.19461886837962084\n",
      "Acc 0.621\n",
      "Epoch 0 - Training loss: 0.18239497298826324\n",
      "Acc 0.632\n",
      "Epoch 0 - Training loss: 0.18104334055062601\n",
      "Acc 0.642\n",
      "Epoch 0 - Training loss: 0.16639492367394268\n",
      "Acc 0.648\n",
      "Epoch 0 - Training loss: 0.1588093928164906\n",
      "Acc 0.658\n",
      "Epoch 0 - Training loss: 0.13114356259639198\n",
      "Acc 0.668\n",
      "Epoch 0 - Training loss: 0.12555074082736514\n",
      "Acc 0.674\n",
      "Epoch 0 - Training loss: 0.128929741644802\n",
      "Acc 0.685\n",
      "Epoch 0 - Training loss: 0.1183107552700676\n",
      "Acc 0.689\n",
      "Epoch 0 - Training loss: 0.09666237296968154\n",
      "Acc 0.692\n",
      "Epoch 0 - Training loss: 0.13419008131382898\n",
      "Acc 0.702\n",
      "Epoch 0 - Training loss: 0.11667996047194613\n",
      "Acc 0.698\n",
      "Epoch 0 - Training loss: 0.11474044480234045\n",
      "Acc 0.696\n",
      "Epoch 0 - Training loss: 0.13623521114803022\n",
      "Acc 0.709\n",
      "Epoch 0 - Training loss: 0.10368484879196015\n",
      "Acc 0.715\n",
      "Epoch 0 - Training loss: 0.11632841492586947\n",
      "Acc 0.718\n",
      "Epoch 0 - Training loss: 0.10734326399445611\n",
      "Acc 0.716\n",
      "Epoch 0 - Training loss: 0.09866680756059228\n",
      "Acc 0.712\n",
      "Epoch 0 - Training loss: 0.10349371130694635\n",
      "Acc 0.721\n",
      "Epoch 0 - Training loss: 0.09851749763623172\n",
      "Acc 0.728\n",
      "Epoch 0 - Training loss: 0.11488276998548266\n",
      "Acc 0.731\n",
      "Epoch 0 - Training loss: 0.12032182998779528\n",
      "Acc 0.735\n",
      "Epoch 0 - Training loss: 0.10267754995988475\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.10027142323617061\n",
      "Acc 0.727\n",
      "Epoch 0 - Training loss: 0.09808118762365277\n",
      "Acc 0.737\n",
      "Epoch 0 - Training loss: 0.09603174013829087\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.09928402725550957\n",
      "Acc 0.741\n",
      "Epoch 0 - Training loss: 0.09725915563837388\n",
      "Acc 0.742\n",
      "Epoch 0 - Training loss: 0.08657519439584575\n",
      "Acc 0.751\n",
      "Epoch 0 - Training loss: 0.07417781512657577\n",
      "Acc 0.753\n",
      "Epoch 0 - Training loss: 0.06825606249021215\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.06817392295315153\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.07170954183129652\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.06967967180893399\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.071422658303301\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.07679241287622796\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.06450564182972919\n",
      "Acc 0.765\n",
      "Epoch 0 - Training loss: 0.05240117707433165\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.05274304104054214\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.05928235267236514\n",
      "Acc 0.776\n",
      "Epoch 0 - Training loss: 0.05317594312631021\n",
      "Acc 0.787\n",
      "Epoch 0 - Training loss: 0.04646995192081094\n",
      "Acc 0.784\n",
      "Epoch 0 - Training loss: 0.04598639133650608\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.04343511040400093\n",
      "Acc 0.794\n",
      "Epoch 0 - Training loss: 0.0385948542042522\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.041315289607344124\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.03328118625004441\n",
      "Acc 0.796\n",
      "Epoch 0 - Training loss: 0.03844355482680287\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.03589143017597962\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.04101037399438756\n",
      "Acc 0.801\n",
      "Epoch 0 - Training loss: 0.03456089231001054\n",
      "Acc 0.804\n",
      "Epoch 0 - Training loss: 0.033917051737536175\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.03476395809307827\n",
      "Acc 0.806\n",
      "Epoch 0 - Training loss: 0.038478255763937555\n",
      "Acc 0.81\n",
      "Epoch 0 - Training loss: 0.036202425467118424\n",
      "Acc 0.809\n",
      "Epoch 0 - Training loss: 0.036634247069616384\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.032612301689368905\n",
      "Acc 0.813\n",
      "Epoch 0 - Training loss: 0.031977361291640416\n",
      "Acc 0.814\n",
      "Epoch 0 - Training loss: 0.05386541024037999\n",
      "Acc 0.813\n",
      "Epoch 0 - Training loss: 0.05188100655840781\n",
      "Acc 0.813\n",
      "Epoch 0 - Training loss: 0.04877149405650055\n",
      "Acc 0.819\n",
      "Epoch 0 - Training loss: 0.04113982306154613\n",
      "Acc 0.818\n",
      "Episode 9 started\n",
      "Epoch 0 - Training loss: 6.693876504898071\n",
      "Acc 0.143\n",
      "Epoch 0 - Training loss: 7.180342435836792\n",
      "Acc 0.151\n",
      "Epoch 0 - Training loss: 5.663351774215698\n",
      "Acc 0.165\n",
      "Epoch 0 - Training loss: 4.384499371051788\n",
      "Acc 0.176\n",
      "Epoch 0 - Training loss: 3.101413881778717\n",
      "Acc 0.226\n",
      "Epoch 0 - Training loss: 2.320104549328486\n",
      "Acc 0.272\n",
      "Epoch 0 - Training loss: 1.7790315034134048\n",
      "Acc 0.295\n",
      "Epoch 0 - Training loss: 1.3315387293696404\n",
      "Acc 0.323\n",
      "Epoch 0 - Training loss: 1.1618288258711498\n",
      "Acc 0.346\n",
      "Epoch 0 - Training loss: 1.0346400432288647\n",
      "Acc 0.384\n",
      "Epoch 0 - Training loss: 0.8567694593220949\n",
      "Acc 0.421\n",
      "Epoch 0 - Training loss: 0.7290333875765403\n",
      "Acc 0.446\n",
      "Epoch 0 - Training loss: 0.5331802190496371\n",
      "Acc 0.475\n",
      "Epoch 0 - Training loss: 0.45751907410366194\n",
      "Acc 0.497\n",
      "Epoch 0 - Training loss: 0.3757278638581435\n",
      "Acc 0.514\n",
      "Epoch 0 - Training loss: 0.35239845304749906\n",
      "Acc 0.543\n",
      "Epoch 0 - Training loss: 0.2822308641146211\n",
      "Acc 0.555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.25912864930513835\n",
      "Acc 0.564\n",
      "Epoch 0 - Training loss: 0.34368440445120396\n",
      "Acc 0.561\n",
      "Epoch 0 - Training loss: 0.3541848157066852\n",
      "Acc 0.568\n",
      "Epoch 0 - Training loss: 0.3464086518312494\n",
      "Acc 0.571\n",
      "Epoch 0 - Training loss: 0.25898941487751226\n",
      "Acc 0.585\n",
      "Epoch 0 - Training loss: 0.244648572543393\n",
      "Acc 0.59\n",
      "Epoch 0 - Training loss: 0.2529952134937048\n",
      "Acc 0.6\n",
      "Epoch 0 - Training loss: 0.22228206194937228\n",
      "Acc 0.615\n",
      "Epoch 0 - Training loss: 0.18359173429556763\n",
      "Acc 0.617\n",
      "Epoch 0 - Training loss: 0.1777164591131387\n",
      "Acc 0.624\n",
      "Epoch 0 - Training loss: 0.12442268660691168\n",
      "Acc 0.644\n",
      "Epoch 0 - Training loss: 0.14655067238571315\n",
      "Acc 0.646\n",
      "Epoch 0 - Training loss: 0.1353764933689187\n",
      "Acc 0.65\n",
      "Epoch 0 - Training loss: 0.12201697317763202\n",
      "Acc 0.66\n",
      "Epoch 0 - Training loss: 0.1199311423843028\n",
      "Acc 0.662\n",
      "Epoch 0 - Training loss: 0.1404277610056328\n",
      "Acc 0.666\n",
      "Epoch 0 - Training loss: 0.14628014632719843\n",
      "Acc 0.677\n",
      "Epoch 0 - Training loss: 0.139905808756261\n",
      "Acc 0.676\n",
      "Epoch 0 - Training loss: 0.1214798973103623\n",
      "Acc 0.685\n",
      "Epoch 0 - Training loss: 0.11953050906241343\n",
      "Acc 0.682\n",
      "Epoch 0 - Training loss: 0.1241137306941183\n",
      "Acc 0.682\n",
      "Epoch 0 - Training loss: 0.10203494668269578\n",
      "Acc 0.691\n",
      "Epoch 0 - Training loss: 0.08375943006249145\n",
      "Acc 0.698\n",
      "Epoch 0 - Training loss: 0.07437281765997773\n",
      "Acc 0.692\n",
      "Epoch 0 - Training loss: 0.06253628239556704\n",
      "Acc 0.697\n",
      "Epoch 0 - Training loss: 0.0636174353286299\n",
      "Acc 0.706\n",
      "Epoch 0 - Training loss: 0.05902770405042578\n",
      "Acc 0.705\n",
      "Epoch 0 - Training loss: 0.05375776588528727\n",
      "Acc 0.713\n",
      "Epoch 0 - Training loss: 0.043290737573482584\n",
      "Acc 0.71\n",
      "Epoch 0 - Training loss: 0.0443006004182067\n",
      "Acc 0.721\n",
      "Epoch 0 - Training loss: 0.042926298495634306\n",
      "Acc 0.72\n",
      "Epoch 0 - Training loss: 0.04164034911497895\n",
      "Acc 0.724\n",
      "Epoch 0 - Training loss: 0.05448072720784694\n",
      "Acc 0.729\n",
      "Epoch 0 - Training loss: 0.04561162113803256\n",
      "Acc 0.73\n",
      "Epoch 0 - Training loss: 0.04073563151737639\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.054765986873908844\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.05455089718021487\n",
      "Acc 0.738\n",
      "Epoch 0 - Training loss: 0.05419083974239501\n",
      "Acc 0.742\n",
      "Epoch 0 - Training loss: 0.06711151289865873\n",
      "Acc 0.746\n",
      "Epoch 0 - Training loss: 0.07644932228855364\n",
      "Acc 0.748\n",
      "Epoch 0 - Training loss: 0.08775717599125747\n",
      "Acc 0.747\n",
      "Epoch 0 - Training loss: 0.08922499493435342\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.06726315839332528\n",
      "Acc 0.757\n",
      "Epoch 0 - Training loss: 0.06640331803908815\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.07295031607669268\n",
      "Acc 0.757\n",
      "Epoch 0 - Training loss: 0.06493663012640484\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.0749518736383834\n",
      "Acc 0.756\n",
      "Epoch 0 - Training loss: 0.05647453271443598\n",
      "Acc 0.765\n",
      "Epoch 0 - Training loss: 0.046934008896531654\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.042767706707662057\n",
      "Acc 0.765\n",
      "Epoch 0 - Training loss: 0.040772533707615154\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.034661008186800325\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.04091415189489323\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.037111436217715045\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.03729162374899412\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.02987000907130208\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.03580568163758742\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.03584413898992352\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.02563742814986893\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.035805782944962984\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.03337743415725596\n",
      "Acc 0.795\n",
      "Epoch 0 - Training loss: 0.036853128932750535\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.03970533029714716\n",
      "Acc 0.798\n",
      "Epoch 0 - Training loss: 0.03763144919494265\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.043617692861694306\n",
      "Acc 0.795\n",
      "Epoch 0 - Training loss: 0.03605297543275898\n",
      "Acc 0.801\n",
      "Epoch 0 - Training loss: 0.031646108887019055\n",
      "Acc 0.801\n",
      "Epoch 0 - Training loss: 0.027201454641523442\n",
      "Acc 0.803\n",
      "Epoch 0 - Training loss: 0.028365044037643602\n",
      "Acc 0.801\n",
      "Epoch 0 - Training loss: 0.023075317220712443\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.02631073234508883\n",
      "Acc 0.805\n",
      "Epoch 0 - Training loss: 0.03300116953912212\n",
      "Acc 0.81\n",
      "Epoch 0 - Training loss: 0.03167127649818819\n",
      "Acc 0.808\n",
      "Epoch 0 - Training loss: 0.037459455973565946\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.04275486034827697\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.0415387370886365\n",
      "Acc 0.81\n",
      "Episode 10 started\n",
      "Epoch 0 - Training loss: 18.187886238098145\n",
      "Acc 0.093\n",
      "Epoch 0 - Training loss: 15.355093479156494\n",
      "Acc 0.091\n",
      "Epoch 0 - Training loss: 11.06955067316691\n",
      "Acc 0.085\n",
      "Epoch 0 - Training loss: 7.8145023584365845\n",
      "Acc 0.11\n",
      "Epoch 0 - Training loss: 4.162769484519958\n",
      "Acc 0.155\n",
      "Epoch 0 - Training loss: 2.4773132602373757\n",
      "Acc 0.232\n",
      "Epoch 0 - Training loss: 1.8556616604328156\n",
      "Acc 0.292\n",
      "Epoch 0 - Training loss: 1.5432371646165848\n",
      "Acc 0.34\n",
      "Epoch 0 - Training loss: 1.4219724469714694\n",
      "Acc 0.378\n",
      "Epoch 0 - Training loss: 1.1808716557919978\n",
      "Acc 0.402\n",
      "Epoch 0 - Training loss: 1.0085975487123837\n",
      "Acc 0.426\n",
      "Epoch 0 - Training loss: 0.8177100401371717\n",
      "Acc 0.44\n",
      "Epoch 0 - Training loss: 0.7007672408452401\n",
      "Acc 0.449\n",
      "Epoch 0 - Training loss: 0.54828177791621\n",
      "Acc 0.472\n",
      "Epoch 0 - Training loss: 0.4899362082282702\n",
      "Acc 0.493\n",
      "Epoch 0 - Training loss: 0.45086973917204887\n",
      "Acc 0.494\n",
      "Epoch 0 - Training loss: 0.3949821383199271\n",
      "Acc 0.522\n",
      "Epoch 0 - Training loss: 0.3435114960496624\n",
      "Acc 0.53\n",
      "Epoch 0 - Training loss: 0.32291186248001297\n",
      "Acc 0.545\n",
      "Epoch 0 - Training loss: 0.2755238416604698\n",
      "Acc 0.56\n",
      "Epoch 0 - Training loss: 0.29965949847939466\n",
      "Acc 0.58\n",
      "Epoch 0 - Training loss: 0.2607025584544648\n",
      "Acc 0.595\n",
      "Epoch 0 - Training loss: 0.21992682795161786\n",
      "Acc 0.6\n",
      "Epoch 0 - Training loss: 0.19108821412858865\n",
      "Acc 0.612\n",
      "Epoch 0 - Training loss: 0.19402038998901844\n",
      "Acc 0.634\n",
      "Epoch 0 - Training loss: 0.18002763737996036\n",
      "Acc 0.635\n",
      "Epoch 0 - Training loss: 0.15719034465857679\n",
      "Acc 0.636\n",
      "Epoch 0 - Training loss: 0.17351703788153827\n",
      "Acc 0.652\n",
      "Epoch 0 - Training loss: 0.1888622636425084\n",
      "Acc 0.653\n",
      "Epoch 0 - Training loss: 0.17566556949168444\n",
      "Acc 0.67\n",
      "Epoch 0 - Training loss: 0.14637430489904457\n",
      "Acc 0.668\n",
      "Epoch 0 - Training loss: 0.15659953786234837\n",
      "Acc 0.67\n",
      "Epoch 0 - Training loss: 0.12177944807053516\n",
      "Acc 0.679\n",
      "Epoch 0 - Training loss: 0.12114264301079161\n",
      "Acc 0.688\n",
      "Epoch 0 - Training loss: 0.13008618181837456\n",
      "Acc 0.681\n",
      "Epoch 0 - Training loss: 0.11897345400777543\n",
      "Acc 0.699\n",
      "Epoch 0 - Training loss: 0.11068085428465761\n",
      "Acc 0.696\n",
      "Epoch 0 - Training loss: 0.0881954467408114\n",
      "Acc 0.704\n",
      "Epoch 0 - Training loss: 0.0952914411154313\n",
      "Acc 0.709\n",
      "Epoch 0 - Training loss: 0.07577040932956151\n",
      "Acc 0.716\n",
      "Epoch 0 - Training loss: 0.0889383650146334\n",
      "Acc 0.714\n",
      "Epoch 0 - Training loss: 0.09838325560780331\n",
      "Acc 0.726\n",
      "Epoch 0 - Training loss: 0.11756897098276504\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.10998821541116657\n",
      "Acc 0.736\n",
      "Epoch 0 - Training loss: 0.08868764386279508\n",
      "Acc 0.74\n",
      "Epoch 0 - Training loss: 0.06810913687830021\n",
      "Acc 0.737\n",
      "Epoch 0 - Training loss: 0.066012164696734\n",
      "Acc 0.742\n",
      "Epoch 0 - Training loss: 0.07232809982330461\n",
      "Acc 0.744\n",
      "Epoch 0 - Training loss: 0.068353728567041\n",
      "Acc 0.74\n",
      "Epoch 0 - Training loss: 0.06191074884030968\n",
      "Acc 0.741\n",
      "Epoch 0 - Training loss: 0.05398926090257352\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.05826352877417006\n",
      "Acc 0.745\n",
      "Epoch 0 - Training loss: 0.05060618986674356\n",
      "Acc 0.752\n",
      "Epoch 0 - Training loss: 0.06015506268832488\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.05431971766520292\n",
      "Acc 0.752\n",
      "Epoch 0 - Training loss: 0.0684792839331619\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.08483322441635098\n",
      "Acc 0.765\n",
      "Epoch 0 - Training loss: 0.0780938113116306\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.07622059000955926\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.06946355811669491\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.07176717559875706\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.06903338831034489\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.07208384548328699\n",
      "Acc 0.782\n",
      "Epoch 0 - Training loss: 0.06874202210110525\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.06056955254887446\n",
      "Acc 0.784\n",
      "Epoch 0 - Training loss: 0.06945159670960327\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.06551912482289264\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.06870379892215543\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.0703491183580743\n",
      "Acc 0.783\n",
      "Epoch 0 - Training loss: 0.06445966885346155\n",
      "Acc 0.789\n",
      "Epoch 0 - Training loss: 0.06670956123574362\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.05985487896476924\n",
      "Acc 0.786\n",
      "Epoch 0 - Training loss: 0.04712071732740709\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.04512592207332066\n",
      "Acc 0.799\n",
      "Epoch 0 - Training loss: 0.04466888194670901\n",
      "Acc 0.797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.05260126403491564\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.04574132944508637\n",
      "Acc 0.808\n",
      "Epoch 0 - Training loss: 0.05668191375876538\n",
      "Acc 0.805\n",
      "Epoch 0 - Training loss: 0.06339989554157623\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.05802470530816208\n",
      "Acc 0.802\n",
      "Epoch 0 - Training loss: 0.06245970173363781\n",
      "Acc 0.816\n",
      "Epoch 0 - Training loss: 0.05955495409526942\n",
      "Acc 0.821\n",
      "Epoch 0 - Training loss: 0.045004311448989816\n",
      "Acc 0.816\n",
      "Epoch 0 - Training loss: 0.04526184238282093\n",
      "Acc 0.819\n",
      "Epoch 0 - Training loss: 0.052810858041287725\n",
      "Acc 0.826\n",
      "Epoch 0 - Training loss: 0.07609858773012515\n",
      "Acc 0.822\n",
      "Epoch 0 - Training loss: 0.07659533979764593\n",
      "Acc 0.828\n",
      "Epoch 0 - Training loss: 0.06355818933033416\n",
      "Acc 0.831\n",
      "Epoch 0 - Training loss: 0.06549926146601975\n",
      "Acc 0.828\n",
      "Epoch 0 - Training loss: 0.05591238141597488\n",
      "Acc 0.828\n",
      "Epoch 0 - Training loss: 0.0597434039908886\n",
      "Acc 0.827\n",
      "Epoch 0 - Training loss: 0.051250534783795716\n",
      "Acc 0.829\n",
      "Epoch 0 - Training loss: 0.05410155643376031\n",
      "Acc 0.831\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "memory_size = 1000\n",
    "num_episodes = 10\n",
    "state_len = 794\n",
    "budget = 1500\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "memory = ReplayMemory(memory_size)\n",
    "dqnet = DQN(state_len)\n",
    "target_dqnet = DQN(state_len)\n",
    "dqnet_optimizer = optim.Adam(dqnet.parameters(), lr=1e-3)\n",
    "num_actions = 2\n",
    "\n",
    "def dqn_train(model, target_model, optimizer, mini_batch):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "    states = np.array([exp[0] for exp in mini_batch])\n",
    "    actions = torch.Tensor([[exp[1]] for exp in mini_batch]).long()\n",
    "    rewards = torch.Tensor([exp[2] for exp in mini_batch])\n",
    "    next_states = torch.Tensor([exp[3] for exp in mini_batch])\n",
    "    model.train()\n",
    "    target_model.eval()\n",
    "    output = model(torch.from_numpy(states).float())\n",
    "    predicted = torch.gather(output, 1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        labels_next = target_model(next_states).detach().max(1).values\n",
    "    labels = rewards + gamma * labels_next\n",
    "    loss = criterion(predicted, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def update_target_net(model, target_model, tau=1e-3):\n",
    "    for target_param, local_param in zip(target_model.parameters(), model.parameters()):\n",
    "        target_param.data.copy_(tau * local_param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "def select_action(state, model, current_step):\n",
    "    eps = strategy.get_exploration_rate(current_step)\n",
    "    if random.random() < eps:\n",
    "        return random.randrange(num_actions) # explore\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            a = model(torch.from_numpy(state).float()).argmax().item() # exploit\n",
    "        model.train()\n",
    "        return a\n",
    "\n",
    "order = list(range(0, train_data.shape[0]))\n",
    "\n",
    "X_labelled = []\n",
    "random.shuffle(order)\n",
    "for _ in range(num_episodes):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    current_step = 0\n",
    "    print('Episode {} started'.format(_ + 1))\n",
    "    model = Net(28*28)\n",
    "    model_optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\n",
    "    prev_acc = predict(model)\n",
    "    while j < train_data.shape[0]:\n",
    "        part1 = np.copy(train_data[order[i]])\n",
    "        part2 = model(torch.from_numpy(train_data[order[i]]).float()).detach().numpy()\n",
    "        state = np.concatenate((part1, part2))\n",
    "        a = select_action(state, dqnet, current_step)\n",
    "        current_step += 1\n",
    "        if a == 1:\n",
    "            X_labelled.append(order[i])\n",
    "            i += 1\n",
    "            if i % 16 == 0:\n",
    "                train(model, model_optimizer, X_labelled)\n",
    "                print('Acc', acc)\n",
    "        acc = predict(model)\n",
    "        r = acc - prev_acc\n",
    "        prev_acc = acc\n",
    "        if i == budget:\n",
    "            X_labelled = []\n",
    "            random.shuffle(order)\n",
    "            i = 0\n",
    "            part1 = np.copy(train_data[order[i]])\n",
    "            part2 = model(torch.from_numpy(train_data[order[i]]).float()).detach().numpy()\n",
    "            new_state = np.concatenate((part1, part2))\n",
    "            memory.push(state, a, r, new_state)\n",
    "            break\n",
    "        part1 = np.copy(train_data[order[i + 1]])\n",
    "        part2 = model(torch.from_numpy(train_data[i + 1]).float()).detach().numpy()\n",
    "        new_state = np.concatenate((part1, part2))\n",
    "        memory.push(state, a, r, new_state)\n",
    "        if i % 4 == 0 and memory.can_provide_sample(batch_size):\n",
    "            mini_batch = memory.sample(batch_size)\n",
    "            dqn_train(dqnet, target_dqnet, dqnet_optimizer, mini_batch)\n",
    "        if i % 8 == 0:\n",
    "            update_target_net(dqnet, target_dqnet)\n",
    "        j += 1\n",
    "#         print('Rem budget: ', budget - i, j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
