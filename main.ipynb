{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    num_points = len(lines)\n",
    "    dim_points = 28 * 28\n",
    "    data = np.empty((num_points, dim_points))\n",
    "    labels = np.empty(num_points)\n",
    "    \n",
    "    for ind, line in enumerate(lines):\n",
    "        num = line.split(',')\n",
    "        labels[ind] = int(num[0])\n",
    "        data[ind] = [ int(x) for x in num[1:] ]\n",
    "        \n",
    "    return (data, labels)\n",
    "\n",
    "train_data, train_labels = read_data(\"sample_train.csv\")\n",
    "test_data, test_labels = read_data(\"sample_test.csv\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = self.fc2(t)\n",
    "        return t\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, strategy):\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = 2\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = self.strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if random.random() < eps:\n",
    "            return random.randrange(self.num_actions) # explore\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim=1).item() # exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.LogSoftmax(self.fc3(x), dim=1)\n",
    "\n",
    "def train(model):\n",
    "    epochs = 10\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(trainloader)))\n",
    "\n",
    "def predict(model):\n",
    "    with torch.no_grad():\n",
    "        output = model(test_data)\n",
    "    softmax = torch.exp(output)\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    return accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "memory_size = 10000\n",
    "num_episodes = 10\n",
    "state_len = 784\n",
    "budget = train_data.shape[0] // 4\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "agent = Agent(strategy)\n",
    "memory = ReplayMemory(memory_size)\n",
    "dqnet = DQN(state_len)\n",
    "\n",
    "order = range(0, train_data.shape[0])\n",
    "\n",
    "X_labelled = []\n",
    "y_labelled = []\n",
    "random.shuffle(order)\n",
    "i = 0\n",
    "for _ in range(num_episodes):\n",
    "    model = Net()\n",
    "    model_optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    prev_acc = predict(model)\n",
    "    while i < budget:\n",
    "        part1 = np.copy(train_data[order[i]])\n",
    "        part2 = model(train_data[order[i]])\n",
    "        state = np.concatenate((part1, part2))\n",
    "        a = np.argmax(dqnet(state))\n",
    "        if a == 1:\n",
    "            y_labelled.append(order[i])\n",
    "            X_labelled.append(order[i])\n",
    "            model.train()\n",
    "            i += 1\n",
    "        acc = predict(model)\n",
    "        r = acc - prev_acc\n",
    "        prev_acc = acc\n",
    "        if i == budget:\n",
    "            X_labelled = []\n",
    "            y_labelled = []\n",
    "            random.shuffle(order)\n",
    "            i = 0\n",
    "            part1 = np.copy(train_data[order[i]])\n",
    "            part2 = model(train_data[order[i]])\n",
    "            new_state = np.concatenate((part1, part2))\n",
    "            memory.push(state, a, r, new_state)\n",
    "            break\n",
    "        part1 = np.copy(train_data[order[i + 1]])\n",
    "        part2 = model(train_data[i + 1])\n",
    "        new_state = np.concatenate((part1, part2))\n",
    "        memory.push(state, a, r, new_state)\n",
    "        if i % 10 == 0 and memory.can_provide_sample(batch_size)\n",
    "            mini_batch = memory.sample(batch_size)\n",
    "            # update DQN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
