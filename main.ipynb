{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 784) (1000, 784)\n",
      "(6000,) (1000,)\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    num_points = len(lines)\n",
    "    dim_points = 28 * 28\n",
    "    data = np.empty((num_points, dim_points))\n",
    "    labels = np.empty(num_points)\n",
    "    \n",
    "    for ind, line in enumerate(lines):\n",
    "        num = line.split(',')\n",
    "        labels[ind] = int(num[0])\n",
    "        data[ind] = [ int(x) for x in num[1:] ]\n",
    "        \n",
    "    return (data, labels)\n",
    "\n",
    "train_data, train_labels = read_data(\"sample_train.csv\")\n",
    "test_data, test_labels = read_data(\"sample_test.csv\")\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "print(type(test_data[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=in_features, out_features=32)\n",
    "        self.fc2 = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = self.fc2(t)\n",
    "        return t\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(self.fc3(x))\n",
    "\n",
    "def train(model, optimizer, X, criterion=nn.NLLLoss()):\n",
    "    epochs = 1\n",
    "    batch_size = 8\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        num_batches = 0\n",
    "        order = np.copy(X)\n",
    "        np.random.shuffle(order)\n",
    "        i = 0\n",
    "        while i < len(X):\n",
    "            j = min(i + batch_size, len(X))\n",
    "#             print(order[i:j])\n",
    "            images = train_data[order[i:j], :]\n",
    "            labels = torch.Tensor(train_labels[order[i:j]]).long()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(torch.from_numpy(images).float())\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            i += batch_size\n",
    "        else:\n",
    "            print(\"Epoch {} - Training loss: {}\".format(e, running_loss / num_batches))\n",
    "\n",
    "def predict(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(torch.from_numpy(test_data).float())\n",
    "    model.train()\n",
    "    softmax = torch.exp(torch.Tensor(output))\n",
    "    prob = list(softmax.numpy())\n",
    "    predictions = np.argmax(prob, axis=1)\n",
    "    return accuracy_score(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 started\n",
      "Epoch 0 - Training loss: 12.53506851196289\n",
      "Acc 0.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 11.260113954544067\n",
      "Acc 0.091\n",
      "Epoch 0 - Training loss: 7.590955813725789\n",
      "Acc 0.103\n",
      "Epoch 0 - Training loss: 5.228154569864273\n",
      "Acc 0.128\n",
      "Epoch 0 - Training loss: 3.5969605684280395\n",
      "Acc 0.162\n",
      "Epoch 0 - Training loss: 2.245310833056768\n",
      "Acc 0.208\n",
      "Epoch 0 - Training loss: 1.6939783011163985\n",
      "Acc 0.239\n",
      "Epoch 0 - Training loss: 1.2592058535665274\n",
      "Acc 0.292\n",
      "Epoch 0 - Training loss: 0.88739213347435\n",
      "Acc 0.319\n",
      "Epoch 0 - Training loss: 0.8695410095155239\n",
      "Acc 0.335\n",
      "Epoch 0 - Training loss: 0.786604565652934\n",
      "Acc 0.37\n",
      "Epoch 0 - Training loss: 0.7303804599990448\n",
      "Acc 0.383\n",
      "Epoch 0 - Training loss: 0.6093187799247411\n",
      "Acc 0.408\n",
      "Epoch 0 - Training loss: 0.4673882730837379\n",
      "Acc 0.436\n",
      "Epoch 0 - Training loss: 0.4018930457532406\n",
      "Acc 0.442\n",
      "Epoch 0 - Training loss: 0.4209327040007338\n",
      "Acc 0.476\n",
      "Epoch 0 - Training loss: 0.4695752798853552\n",
      "Acc 0.491\n",
      "Epoch 0 - Training loss: 0.4079093835834\n",
      "Acc 0.513\n",
      "Epoch 0 - Training loss: 0.42673396211313575\n",
      "Acc 0.533\n",
      "Epoch 0 - Training loss: 0.447476811427623\n",
      "Acc 0.548\n",
      "Epoch 0 - Training loss: 0.38591939046801554\n",
      "Acc 0.563\n",
      "Epoch 0 - Training loss: 0.2964362725192173\n",
      "Acc 0.582\n",
      "Epoch 0 - Training loss: 0.2744017638590025\n",
      "Acc 0.598\n",
      "Epoch 0 - Training loss: 0.2635683920234442\n",
      "Acc 0.601\n",
      "Epoch 0 - Training loss: 0.2125889553502202\n",
      "Acc 0.607\n",
      "Epoch 0 - Training loss: 0.1997600749063377\n",
      "Acc 0.625\n",
      "Epoch 0 - Training loss: 0.2006386295650844\n",
      "Acc 0.634\n",
      "Epoch 0 - Training loss: 0.21215038811455347\n",
      "Acc 0.642\n",
      "Epoch 0 - Training loss: 0.19707329315936256\n",
      "Acc 0.623\n",
      "Epoch 0 - Training loss: 0.17873816705929738\n",
      "Acc 0.645\n",
      "Epoch 0 - Training loss: 0.19874680557498528\n",
      "Acc 0.651\n",
      "Epoch 0 - Training loss: 0.19687241607607575\n",
      "Acc 0.657\n",
      "Epoch 0 - Training loss: 0.15479713372152412\n",
      "Acc 0.664\n",
      "Epoch 0 - Training loss: 0.13356187024756389\n",
      "Acc 0.669\n",
      "Epoch 0 - Training loss: 0.1195612628678126\n",
      "Acc 0.675\n",
      "Epoch 0 - Training loss: 0.11072629858325753\n",
      "Acc 0.676\n",
      "Epoch 0 - Training loss: 0.09706243400688509\n",
      "Acc 0.679\n",
      "Epoch 0 - Training loss: 0.09350878402198616\n",
      "Acc 0.686\n",
      "Epoch 0 - Training loss: 0.07889389973253202\n",
      "Acc 0.688\n",
      "Epoch 0 - Training loss: 0.09099977352307179\n",
      "Acc 0.692\n",
      "Epoch 0 - Training loss: 0.09074395592608375\n",
      "Acc 0.706\n",
      "Epoch 0 - Training loss: 0.09964972176212109\n",
      "Acc 0.698\n",
      "Epoch 0 - Training loss: 0.11762837723417338\n",
      "Acc 0.7\n",
      "Epoch 0 - Training loss: 0.11183859348635782\n",
      "Acc 0.701\n",
      "Epoch 0 - Training loss: 0.11351683768443763\n",
      "Acc 0.701\n",
      "Epoch 0 - Training loss: 0.09836208795541493\n",
      "Acc 0.713\n",
      "Epoch 0 - Training loss: 0.0887906306641216\n",
      "Acc 0.715\n",
      "Epoch 0 - Training loss: 0.08912162830165471\n",
      "Acc 0.717\n",
      "Epoch 0 - Training loss: 0.07568848346436054\n",
      "Acc 0.721\n",
      "Epoch 0 - Training loss: 0.055085194441489876\n",
      "Acc 0.724\n",
      "Epoch 0 - Training loss: 0.06917204253384661\n",
      "Acc 0.726\n",
      "Epoch 0 - Training loss: 0.05820408953765694\n",
      "Acc 0.731\n",
      "Epoch 0 - Training loss: 0.06030290605025893\n",
      "Acc 0.745\n",
      "Epoch 0 - Training loss: 0.06654650323239535\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.06803146046956747\n",
      "Acc 0.754\n",
      "Epoch 0 - Training loss: 0.0614250179460214\n",
      "Acc 0.751\n",
      "Epoch 0 - Training loss: 0.05896219116709146\n",
      "Acc 0.756\n",
      "Episode 2 started\n",
      "Epoch 0 - Training loss: 4.406585185955732\n",
      "Acc 0.146\n",
      "Epoch 0 - Training loss: 1.1276632986587136\n",
      "Acc 0.495\n",
      "Epoch 0 - Training loss: 0.6325640343567395\n",
      "Acc 0.603\n",
      "Epoch 0 - Training loss: 0.4081591513477326\n",
      "Acc 0.657\n",
      "Epoch 0 - Training loss: 0.28782390566170213\n",
      "Acc 0.682\n",
      "Epoch 0 - Training loss: 0.20672404383287185\n",
      "Acc 0.709\n",
      "Epoch 0 - Training loss: 0.15327845418578928\n",
      "Acc 0.719\n",
      "Epoch 0 - Training loss: 0.12315429269535225\n",
      "Acc 0.718\n",
      "Epoch 0 - Training loss: 0.09596927660426363\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.07667174812544275\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.06575146056269805\n",
      "Acc 0.749\n",
      "Epoch 0 - Training loss: 0.05833123320472433\n",
      "Acc 0.744\n",
      "Epoch 0 - Training loss: 0.049869473626086215\n",
      "Acc 0.75\n",
      "Epoch 0 - Training loss: 0.04394429651336497\n",
      "Acc 0.756\n",
      "Epoch 0 - Training loss: 0.03904476250553953\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.035201207549330225\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.03724942436979321\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.03130875415221128\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.028030430603988493\n",
      "Acc 0.765\n",
      "Epoch 0 - Training loss: 0.024700572396132856\n",
      "Acc 0.764\n",
      "Epoch 0 - Training loss: 0.022038520924794445\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.020228252954833773\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.01864524171485201\n",
      "Acc 0.767\n",
      "Epoch 0 - Training loss: 0.017165442340167387\n",
      "Acc 0.767\n",
      "Epoch 0 - Training loss: 0.016087185576290006\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.01500007746501374\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.014215987229433198\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.013359491248561346\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.012743062866620202\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.012240474698732474\n",
      "Acc 0.771\n",
      "Epoch 0 - Training loss: 0.01142222517351541\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.01089887530412837\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.01048188367564275\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.010021389598030688\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.009600197644652548\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.009163891670000266\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.00879918294466007\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.008447683352766149\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.00815428259955608\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.0078796690448097\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.007591441814695499\n",
      "Acc 0.779\n",
      "Episode 3 started\n",
      "Epoch 0 - Training loss: 2.5926028263928305\n",
      "Acc 0.109\n",
      "Epoch 0 - Training loss: 0.6339682820832255\n",
      "Acc 0.562\n",
      "Epoch 0 - Training loss: 0.3258396958831349\n",
      "Acc 0.66\n",
      "Epoch 0 - Training loss: 0.1995708561343391\n",
      "Acc 0.698\n",
      "Epoch 0 - Training loss: 0.13201767381658588\n",
      "Acc 0.724\n",
      "Epoch 0 - Training loss: 0.09568306240676479\n",
      "Acc 0.727\n",
      "Epoch 0 - Training loss: 0.07372942065127124\n",
      "Acc 0.733\n",
      "Epoch 0 - Training loss: 0.05739900632214868\n",
      "Acc 0.744\n",
      "Epoch 0 - Training loss: 0.046883440424883085\n",
      "Acc 0.743\n",
      "Epoch 0 - Training loss: 0.039979579170719474\n",
      "Acc 0.744\n",
      "Epoch 0 - Training loss: 0.03430239626017216\n",
      "Acc 0.749\n",
      "Epoch 0 - Training loss: 0.03018737713410203\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.026758395377108033\n",
      "Acc 0.756\n",
      "Epoch 0 - Training loss: 0.024159653212668167\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.021779688808037222\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.01988094621470125\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.018562824599388423\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.017056523757794866\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.015894991616183456\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.014816886153246988\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.013778985669411657\n",
      "Acc 0.764\n",
      "Epoch 0 - Training loss: 0.012949551569676622\n",
      "Acc 0.767\n",
      "Epoch 0 - Training loss: 0.012200241673684107\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.011494764634313023\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.010857584071345627\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.010352486395043317\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.009816875275450785\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.009399469387616315\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.009004520269010363\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.008558733453557388\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.00819791984114199\n",
      "Acc 0.776\n",
      "Epoch 0 - Training loss: 0.007872060750958143\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.007539780318698078\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.007202932914885921\n",
      "Acc 0.778\n",
      "Episode 4 started\n",
      "Epoch 0 - Training loss: 2.1999748947182254\n",
      "Acc 0.091\n",
      "Epoch 0 - Training loss: 0.4280677411222347\n",
      "Acc 0.597\n",
      "Epoch 0 - Training loss: 0.2150588071980404\n",
      "Acc 0.651\n",
      "Epoch 0 - Training loss: 0.13280508669949795\n",
      "Acc 0.677\n",
      "Epoch 0 - Training loss: 0.09156934945992586\n",
      "Acc 0.707\n",
      "Epoch 0 - Training loss: 0.06863826721749208\n",
      "Acc 0.711\n",
      "Epoch 0 - Training loss: 0.05396065065371139\n",
      "Acc 0.723\n",
      "Epoch 0 - Training loss: 0.04418729483859997\n",
      "Acc 0.721\n",
      "Epoch 0 - Training loss: 0.037527774112187585\n",
      "Acc 0.724\n",
      "Epoch 0 - Training loss: 0.031350438021468585\n",
      "Acc 0.729\n",
      "Epoch 0 - Training loss: 0.026910789998105936\n",
      "Acc 0.734\n",
      "Epoch 0 - Training loss: 0.02363941357310476\n",
      "Acc 0.735\n",
      "Epoch 0 - Training loss: 0.02089796662019655\n",
      "Acc 0.735\n",
      "Epoch 0 - Training loss: 0.018872938830829956\n",
      "Acc 0.739\n",
      "Epoch 0 - Training loss: 0.016898278246879124\n",
      "Acc 0.739\n",
      "Epoch 0 - Training loss: 0.015495026823220737\n",
      "Acc 0.741\n",
      "Epoch 0 - Training loss: 0.014309935349738226\n",
      "Acc 0.747\n",
      "Epoch 0 - Training loss: 0.0131334230413241\n",
      "Acc 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.012170396359962117\n",
      "Acc 0.748\n",
      "Epoch 0 - Training loss: 0.01131055362483036\n",
      "Acc 0.752\n",
      "Epoch 0 - Training loss: 0.01052278679679904\n",
      "Acc 0.753\n",
      "Epoch 0 - Training loss: 0.009866288952311621\n",
      "Acc 0.759\n",
      "Epoch 0 - Training loss: 0.009292592380239885\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.008732479669607487\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.008223716153155739\n",
      "Acc 0.758\n",
      "Epoch 0 - Training loss: 0.00782501683610714\n",
      "Acc 0.762\n",
      "Epoch 0 - Training loss: 0.007459193749036785\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.007084983333573613\n",
      "Acc 0.76\n",
      "Epoch 0 - Training loss: 0.00677012013185936\n",
      "Acc 0.761\n",
      "Epoch 0 - Training loss: 0.006448244944713336\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.006184007393241655\n",
      "Acc 0.763\n",
      "Epoch 0 - Training loss: 0.005980778183209513\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.005717191845178604\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.005474559103620091\n",
      "Acc 0.766\n",
      "Epoch 0 - Training loss: 0.0052697934018700225\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.005077921694141851\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.004898606218579089\n",
      "Acc 0.77\n",
      "Epoch 0 - Training loss: 0.004726169844823808\n",
      "Acc 0.769\n",
      "Epoch 0 - Training loss: 0.00457250612935452\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.0044205845556961445\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.004269280166237417\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.004145246978732757\n",
      "Acc 0.772\n",
      "Epoch 0 - Training loss: 0.004079923441498646\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.003943146277964904\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.0038356513668373594\n",
      "Acc 0.773\n",
      "Epoch 0 - Training loss: 0.003748647636127809\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.0036358536340736058\n",
      "Acc 0.774\n",
      "Epoch 0 - Training loss: 0.0035395611591929137\n",
      "Acc 0.775\n",
      "Epoch 0 - Training loss: 0.003451842682997077\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.0033598141324079132\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.0032779254817559495\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.003201822628663551\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.003161424042406897\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.0031047018482906986\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.0030959833362972627\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.0030545641146273793\n",
      "Acc 0.78\n",
      "Epoch 0 - Training loss: 0.002981037544151676\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.011638014375421335\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.009822863834112164\n",
      "Acc 0.778\n",
      "Epoch 0 - Training loss: 0.016378574152615066\n",
      "Acc 0.776\n",
      "Epoch 0 - Training loss: 0.016249067926033492\n",
      "Acc 0.779\n",
      "Epoch 0 - Training loss: 0.013483017451625738\n",
      "Acc 0.777\n",
      "Epoch 0 - Training loss: 0.017219219082516227\n",
      "Acc 0.781\n",
      "Epoch 0 - Training loss: 0.014766423320214543\n",
      "Acc 0.785\n",
      "Epoch 0 - Training loss: 0.011988499985524656\n",
      "Acc 0.789\n",
      "Epoch 0 - Training loss: 0.010709999057988756\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.01045706009515925\n",
      "Acc 0.788\n",
      "Epoch 0 - Training loss: 0.01624669493317942\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.021310799924213995\n",
      "Acc 0.789\n",
      "Epoch 0 - Training loss: 0.02953809484600874\n",
      "Acc 0.79\n",
      "Epoch 0 - Training loss: 0.025811671944621577\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.022273564232951997\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.01924060278140388\n",
      "Acc 0.791\n",
      "Epoch 0 - Training loss: 0.02040468164151484\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.017014273574239687\n",
      "Acc 0.792\n",
      "Epoch 0 - Training loss: 0.02233353682370209\n",
      "Acc 0.793\n",
      "Epoch 0 - Training loss: 0.020572576903179565\n",
      "Acc 0.794\n",
      "Epoch 0 - Training loss: 0.01900434401324234\n",
      "Acc 0.802\n",
      "Epoch 0 - Training loss: 0.01592416546204817\n",
      "Acc 0.796\n",
      "Epoch 0 - Training loss: 0.017069329572363977\n",
      "Acc 0.8\n",
      "Epoch 0 - Training loss: 0.02457893117440176\n",
      "Acc 0.808\n",
      "Epoch 0 - Training loss: 0.020009494186752966\n",
      "Acc 0.809\n",
      "Epoch 0 - Training loss: 0.01485380290786351\n",
      "Acc 0.807\n",
      "Epoch 0 - Training loss: 0.015100716487831792\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.013706562894986504\n",
      "Acc 0.816\n",
      "Epoch 0 - Training loss: 0.01387991353940005\n",
      "Acc 0.814\n",
      "Epoch 0 - Training loss: 0.012957571981976137\n",
      "Acc 0.811\n",
      "Epoch 0 - Training loss: 0.012546129756715365\n",
      "Acc 0.821\n",
      "Epoch 0 - Training loss: 0.01473993511448547\n",
      "Acc 0.82\n",
      "Epoch 0 - Training loss: 0.013283760280603117\n",
      "Acc 0.824\n",
      "Epoch 0 - Training loss: 0.012927084605475361\n",
      "Acc 0.82\n",
      "Epoch 0 - Training loss: 0.012240882991480045\n",
      "Acc 0.821\n",
      "Epoch 0 - Training loss: 0.013956373982109976\n",
      "Acc 0.821\n",
      "Episode 5 started\n",
      "Epoch 0 - Training loss: 14.523021697998047\n",
      "Acc 0.113\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001\n",
    "memory_size = 1000\n",
    "num_episodes = 10\n",
    "state_len = 794\n",
    "budget = 1500\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "memory = ReplayMemory(memory_size)\n",
    "dqnet = DQN(state_len)\n",
    "target_dqnet = DQN(state_len)\n",
    "dqnet_optimizer = optim.Adam(dqnet.parameters(), lr=1e-3)\n",
    "num_actions = 2\n",
    "\n",
    "def dqn_train(model, target_model, optimizer, mini_batch):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "    states = np.array([exp[0] for exp in mini_batch])\n",
    "    actions = torch.Tensor([[exp[1]] for exp in mini_batch]).long()\n",
    "    rewards = torch.Tensor([exp[2] for exp in mini_batch])\n",
    "    next_states = torch.Tensor([exp[3] for exp in mini_batch])\n",
    "    model.train()\n",
    "    target_model.eval()\n",
    "    output = model(torch.from_numpy(states).float())\n",
    "    predicted = torch.gather(output, 1, actions).squeeze()\n",
    "    with torch.no_grad():\n",
    "        labels_next = target_model(next_states).detach().max(1).values\n",
    "    labels = rewards + gamma * labels_next\n",
    "    loss = criterion(predicted, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def update_target_net(model, target_model, tau=1e-3):\n",
    "    for target_param, local_param in zip(target_model.parameters(), model.parameters()):\n",
    "        target_param.data.copy_(tau * local_param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "def select_action(state, model, current_step):\n",
    "    eps = strategy.get_exploration_rate(current_step)\n",
    "    if random.random() < eps:\n",
    "        return random.randrange(num_actions) # explore\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            a = model(torch.from_numpy(state).float()).argmax().item() # exploit\n",
    "        model.train()\n",
    "        return a\n",
    "\n",
    "order = list(range(0, train_data.shape[0]))\n",
    "\n",
    "plots = []\n",
    "X_labelled = []\n",
    "random.shuffle(order)\n",
    "for _ in range(num_episodes):\n",
    "    plot_data = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    current_step = 0\n",
    "    print('Episode {} started'.format(_ + 1))\n",
    "    model = Net(28*28)\n",
    "    model_optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)\n",
    "    prev_acc = predict(model)\n",
    "    while j < train_data.shape[0]:\n",
    "        part1 = np.copy(train_data[order[i]])\n",
    "        part2 = model(torch.from_numpy(train_data[order[i]]).float()).detach().numpy()\n",
    "        state = np.concatenate((part1, part2))\n",
    "        a = select_action(state, dqnet, current_step)\n",
    "        current_step += 1\n",
    "        if a == 1:\n",
    "            X_labelled.append(order[i])\n",
    "            i += 1\n",
    "            if i % 16 == 0:\n",
    "                train(model, model_optimizer, X_labelled)\n",
    "                print('Acc', acc)\n",
    "                plot_data.append([len(X_labelled), acc])\n",
    "        acc = predict(model)\n",
    "        r = acc - prev_acc\n",
    "        prev_acc = acc\n",
    "        if i == budget:\n",
    "            X_labelled = []\n",
    "            random.shuffle(order)\n",
    "            i = 0\n",
    "            part1 = np.copy(train_data[order[i]])\n",
    "            part2 = model(torch.from_numpy(train_data[order[i]]).float()).detach().numpy()\n",
    "            new_state = np.concatenate((part1, part2))\n",
    "            memory.push(state, a, r, new_state)\n",
    "            break\n",
    "        part1 = np.copy(train_data[order[i + 1]])\n",
    "        part2 = model(torch.from_numpy(train_data[i + 1]).float()).detach().numpy()\n",
    "        new_state = np.concatenate((part1, part2))\n",
    "        memory.push(state, a, r, new_state)\n",
    "        if i % 4 == 0 and memory.can_provide_sample(batch_size):\n",
    "            mini_batch = memory.sample(batch_size)\n",
    "            dqn_train(dqnet, target_dqnet, dqnet_optimizer, mini_batch)\n",
    "        if i % 8 == 0:\n",
    "            update_target_net(dqnet, target_dqnet)\n",
    "        j += 1\n",
    "#         print('Rem budget: ', budget - i, j)\n",
    "    plots.append(plot_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
